[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to the blog! Here you‚Äôll find posts on technical writing, machine learning, and more.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan LLMs Survive in the Wild?\n\n\n\n\n\n\n\n\nFeb 22, 2026\n\n\nRitwik Raha\n\n\n\n\n\n\n\n\n\n\n\n\nMathematical Mysteries of the Starry Night\n\n\n\n\n\n\n\n\nFeb 21, 2026\n\n\nRitwik Raha\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Grey Wolf Optimization\n\n\n\n\n\n\n\n\nDec 12, 2025\n\n\nRitwik Raha\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on Technical Writing\n\n\n\n\n\n\n\n\nDec 29, 2024\n\n\nRitwik Raha\n\n\n\n\n\n\n\n\n\n\n\n\nMixture of Experts for Dummies\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nRitwik Raha\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding AugMix\n\n\n\n\n\n\n\n\nJul 24, 2023\n\n\nRitwik Raha\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Computational Photography\n\n\n\n\n\n\n\n\nJan 18, 2022\n\n\nRitwik Raha\n\n\n\n\n\n\n\n\n\n\n\n\nThe Math of Photoshop Blend Modes\n\n\n\n\n\n\n\n\nNov 9, 2021\n\n\nRitwik Raha\n\n\n\n\n\n\n\n\n\n\n\n\nWhy model decay using the exponential function?\n\n\n\n\n\n\n\n\nSep 14, 2021\n\n\nRitwik Raha\n\n\n\n\n\n\n\n\n\n\n\n\nA Brief Introduction to Do-Calculus\n\n\n\n\n\n\n\n\nAug 10, 2021\n\n\nRitwik Raha\n\n\n\n\n\n\n\n\n\n\n\n\nStudying Causality: The good, the bad, and the ugly.\n\n\n\n\n\n\n\n\nApr 28, 2021\n\n\nRitwik Raha\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do we mean when we talk about Causal Inference?\n\n\n\n\n\n\n\n\nApr 12, 2021\n\n\nRitwik Raha\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-11-09-the-math-of-photoshop-blend-modes.html",
    "href": "posts/2021-11-09-the-math-of-photoshop-blend-modes.html",
    "title": "The Math of Photoshop Blend Modes",
    "section": "",
    "text": "Photoshop is a wonderful tool for working with images. It is a delight to work with as a designer and it is an instrument to marvel at as a Computer Vision engineer.\nBut the most used feature of Photoshop (atleast for me) is the blend-modes. From time immemorial I have wondered how these work and how do they create beautiful combination of images.\n\n\n\nblend-modes\n\n\nIn this blog we will learn the following:\n\nHow do photoshop blend modes work?\nThe math behind some blend modes\nRecreating blend modes using python\n\n\nUnder the hood of blend modes\nTo begin with, let us first try to understand what is a blend mode. The idea is to blend two different images to produce a third image. Now there are different rules for blending and each of them results in a different output image. The easiest way to think of this is as a function.\n\\[o = f(x,y)\\]\nwhere \\[x\\] and \\[y\\] are the input images, \\[o\\] is the output image, and the function is the process of blending.\n\n\n\nblend-modes-operation\n\n\nNote: x and y represent color(RGB) values of the image\nAs we devise different functions we will create different blend modes. Sounds simple right?\nLet us go through some blend modes and understand how they work:\n\n\nSome simple blend modes\nIn this blog we will look at four simple blend modes:\n\nNormal\nMultiply\nScreen\nOverlay\n\nWe will also cover the mathematical intution for these modes and how to easily code them up using python. We load and display the images using OpenCV and matplotlib. For the blending operations we use numpy.\nYou can download the entire source code of this blogpost from here.\nNormal\nThe first one is always the most simple one. When two images are placed over each other this mode will choose to show only the top image. Mathematically we can express this like:\n\\[f(x,y) = y\\]\nThis is also called alpha-composting. It is relatively easy to code.\ndef normal(imgA,imgB):\n  # make a copy of the second image\n  imgBlended = np.copy(imgB)\n  # convert the image back into uint8 \n  imgOut=(imgBlended*255).astype(np.uint8)\n  # return the blended image\n  return imgOut\nWe take two images and then create a copy of the second image. This image is passed back as output.\nMultiply\nNext we have the second most used blending mode. Before we go into the theory, imagine this:\nYou have the scanned signature of your parent and you want to place it on your leave application. You open up your image editing software and place the two images as you want them. But it does not look real. Something seems off.\nThis is where multiply comes in. This takes the value of each pixel of the first image and multiplies it with each corresponding pixel of the second image. The output image is darker across all pixels than either of the previous values.\nMathematically we can express this as:\n\\[f(x,y) = xy\\]\nThis is also quite easy to code up.\ndef multiply(imgA,imgB):\n  # create a container for the blended image\n  imgBlended = np.zeros_like(imgA)\n  # apply the blending formula to the images\n  imgBlended = imgA*imgB\n  # convert the image back into uint8\n  imgOut=(imgBlended*255).astype(np.uint8)\n  # return th blended image\n  return imgOut\nIn the above method we first create a container to hold the blended image. Then we store the product of the two images and store it in imgBlended. It is then converted back into uint8 format and passed back.\nScreen\nNow the multiply blend mode makes the composite image look darker. What if we want the composite image to be brighter instead?\nYes we can simply invert what we did in the multiply blend mode to achieve that. First we invert the two images and multiply them. Then we invert the result. The formula would look something like this:\n\\[f(x,y) = (1-(1-x)(1-y))\\]\nLet us see how to express this in code:\ndef screen(imgA,imgB):\n  # create a container for the blended image\n  imgBlended = np.zeros_like(imgA)\n  # apply the blending formula to the images\n  imgBlended = (1-(1-imgA)*(1-imgB))\n  # convert the image back into uint8\n  imgOut=(imgBlended*255).astype(np.uint8)\n  # return the blended image\n  return imgOut\nIn the above method we first create a container for the output image. Then we apply the operation to the two images and store the output in imgBlended. It is then converted back into uint8 format and passed back.\nOverlay\nLife is not seen in only light and dark and neither are images. While darkening and brightening an image are quite useful, it is also necessary to be adaptive. Overlay brings in the best of both blending modes.\nWhen the pixels of the first image is dark the pixels of the composite image is darker, when the pixels of the first image is light the pixels of the composite image is lighter. Usually the threshold is set at 0.5. The formula can be expressed as:\n\\[f(x,y) = \\begin{cases}\n    2xy, & \\text{if $x&lt;0.5$}.\\\\\n    1-2(1-a)(1-b), & \\text{otherwise}.\n  \\end{cases}\\]\nLet us see how we can code this up:\ndef overlay(imgA,imgB):\n  # create a mask of the image A everywhere\n  # the pixels are greater than 0.5\n  mask = imgA &gt;= 0.5\n  # create a container for the blended image\n  imgBlended = np.zeros_like(imgA)\n  # apply the blending formula to the mask\n  imgBlended[~mask] = (2*imgA*imgB)[~mask]\n  imgBlended[mask] = (1-2*(1-imgA)*(1-imgB))[mask]\n  # convert the image back into uint8\n  imgOut=(imgBlended*255).astype(np.uint8)\n  # return the blended image\n  return imgOut\nIn the above method we create a mask for the threshold(wherever the first image has values over 0.5). Next we create a container for the blended image and store the belnd operation values in it. For everywhere other than the mask values we apply the multiply like operation and for everywhere else we apply the screen like operation.\n\n\nSource Code\nYou can download the entire source code of this blogpost from here.\n\n\nConclusion\nThere we have it! Our first batch of photoshop like blend modes are now ready. And the best thing?\nWe built them from scracth!\nLet‚Äôs see what our result looks like:\n\n\n\nblend-modes-result\n\n\nNow, needless to say this barely scratches the surface. Real image processing applications like Photoshop have a lot going on under the hood. The calculations are much more streamlined and sophisticated. They also have an array of other blend modes.\n\n\n\nblend-modes-result\n\n\nThis repository aims to faithfully replicate these calculations to some degree. The actual literature of the blnd modes used by Adobe is also provided here.\nHowever what we learned here today gives us a good starting point to understand and minimally recreate blend-modes.\n\n\nReferences\n\nImage Blend Modes - Wikipedia\nAdobe Blend Modes Gudielines\nBlending Modes Explained - Photoshop training channel\nThe Math behind Blend Modes - Imagineer\nBlend Modes in Python\nPillow Blend Modes"
  },
  {
    "objectID": "posts/2021-04-28-studying-causality-the-good-the-bad-and-the-ugly.html",
    "href": "posts/2021-04-28-studying-causality-the-good-the-bad-and-the-ugly.html",
    "title": "Studying Causality: The good, the bad, and the ugly.",
    "section": "",
    "text": "Welcome back to Part 2 of The Causal Blog. The previous one was all about introducing Causality in the briefest way possible. You can find it right here.\nPicking up where we left off let us revise what we learned in Part 1.\n\nIgnoring hidden causes in data can mean death for your model\nCounterfactual thinking is a must\nRandomization, Natural Experiments and Conditioning are the tools of the trade\n\nRight. Now that we have got that sorted, let us start with those three vague terms. Randomization, natural experiments, and conditioning, what are they? How do we use them with our data and why do we need to care?\nWell let us start scratching the list one item at a time.\n\nRandomization\nSo what do we mean when we talk about Randomization in the context of Causality. Well at the very core of it causal knowledge, prefers intervention, in fact, it demands it. What do I mean by that?\nIn very simple terms if you say A causes B, you must be able to show to some degree that in the absence of A and all other parameters staying constant B will not occur.\nNow that is a fairly straightforward statement but the implications go deeper. Say suppose you want to prove that ads by ads on Netflix will cause users to delete accounts. That‚Äôs a decent assumption, and you start trying to prove it. You remove the cause, (Netflix ads) and try to study the behavior of the user, but here‚Äôs the catch, you are not exactly studying user behavior. The other parameters are not kept constant. You are not looking at a parallel world where a doppelganger of the user is not shown an ad when they open Netflix.\nThis is exactly what randomization tries to solve. It basically tells us that in the context of our experiment (ads on Netflix) if we create a set of users and then randomly assign a user to a group (one group gets shown ads and the other group doesn‚Äôt) then we are essentially keeping the ‚Äúother parameters‚Äù constant.\n\n\n\nexample1\n\n\nTwo ways we can achieve randomization are:\n\nA/B testing\nMulti-armed bandits\n\n\nA/B Test\nThis is exactly what we were talking about a second back. Create two groups of subjects (A & B group) and treat them differently (ads vs no ads) and see which one of the groups meets the target (cancellation of subscription). So in summary the steps to be followed are :\n\nCreate two groups of subjects\nExpose each of the groups to two different experiments\nFind out which meets the target\n\nThe Causal Estimate (CE) for this methodology is the difference in outcome due to option 1 and option 2.\n\\[\nCE = Y_{option1} - Y_{option2}\n\\]\n\nCheck out how Netflix actually uses A/B testing in this video.\n\n\n\nMulti-armed bandits\nThe second and a little less famous of the randomization strategy is multi-armed bandits. This methodology derives its name from the arms of slot machines. Historically, the infamous problem goes as such:\nGiven multiple slot machines, a gambler has to decide which arms to pull and how many times in order to maximize profit. Needless to say, countless have been slain by this problem, purely to quench their mathematical curiosity. üòâ\nThis method relies on a very interesting strategy known as Explore & Exploit. Explore as in explore all possible options in the given context of the problem and Exploit as in figure out the best option and keep repeating it to maximize target outcome.\nOne example of this would if you had to figure out which Baskin Robbin‚Äôs ice cream would give you the maximum satisfaction (target outcome). The steps listed down below are some of the ways to carry out Exploration & Exploitation while maximizing satisfaction.\n\n\n\nexample2\n\n\n\nExplore only. This basically means everyday you go to Baskin Robbins and select a new flavor and try it out and see if it gives you maximum satisfaction.\nExploit only. On the first 10 days, you go to Baskin Robbins and try out 10 different flavors. From the 11th day, you keep buying that one flavor that gave you maximum satisfaction from those 10 previously tested ones.\n\nEpsilon - greedy.\n\nStrike a balance between explore an exploit\nSet an initial epsilon (indicator)\nBased on the value of epsilon (randomly picked) we will either explore or exploit.\nIf we exploit, then we pick the best ice cream from already gathered data, if we explore then we randomly pick an ice cream from the counter.\n\n\\[Y_b\\] = target outcome is the best possible, \\[Y_c\\] = current target outcome and \\[CE\\] = Causal Estimate\n\n\n\\[\nCE = Y_b - Y_c\n\\]\n\nUse multi-armed bandits only and only if you have a good number of options to test.\n\n\nThe verdict?\n\n\nUse Randomization even it means staying up a couple of nights and designing an experiment in which you have to create a parallel universe. In all seriousness, the two methods shown above actually gives us a good structure through which we can shape randomized experiments that give us a sense of causal direction.\n\n\n\n\nNatural Experiments\nYep, randomization is hard and if there is absolutely no way for you to intervene, then that ship has sailed. So let us have a look at what the next best option is. Natural Experiments is a way to test causality by shaping naturally occurring phenomenon as an experiment. Its kind of like cheating but in a scientific way. ü§£\nThe two most prevalent ways to do this are as follows:\n\nRegression Discontinuity\nInstrumental Variables\n\n\nRegression Discontinuity\nLet us imagine that we are the top-secret evil society that controls the funding for academic labs. We want to check if granting more funds will cause an increase in lab performance, but there is a catch funding is also dependent on the number of papers published i.e.¬†number of papers published will have to be above a certain threshold (theta) for the lab to qualify for funding.\n\n\n\nexample2\n\n\n\\[\ny = B_0 +B_1x +B_2(x&gt;x_t)+e\n\\]\nUse the linear regression model to estimate the outcome however and this may be counter-intuitive if there is a discontinuous jump at the point where \\(x =x_t\\) or when your threshold is reached. The measure of this discontinuity is your Causal Estimate. If there is a small jump it means funding probably does not play a big role in lab performance and we can continue to be greedy evil super-villains. If the jump is huge enough, then we have got problems and might have to release some of our super-villain money as research grants.\n\n\nInstrumental Variable\nBefore we dive deep into this particular let us revise some of the concepts we glided past previously.\nOutcome Variable. This is a notation (almost always Y) used to denote the final outcome of our causal experiments.\nTreatment Variable. This is another useful notation (almost always t) used to denote the treatment in our causal experiments. In the context of the Netflix ads problem t = showing ads on Netflix or t = not showing ads on Netflix.\nInstrumental Variable = This is used to denote the variable which affects our outcome through our treatment variable. This variable does not have the scope to affect the outcome directly but influences the treatment variable and thereby indirectly affects the outcome.\nConfused? Let‚Äôs take a look at an example:\nImagine we have now joined a publishing company as a data scientist. A strange rumor is abuzz in the marketing department. It is believed that more Twitter mentions will always cause more book sales. As data scientists only we can debunk or solidify this rumor.\n Graph for scenario 1\n Graph for scenario 2\nConsider the following:\nLet us try to model the two causal graphs shown above using simple regression equations. A list of notations is given below:\n\n\\[bs_{orig}\\] = Original figure for book sales.\n\\[tm\\] = Original number of twitter mentions.\n\\[\\hat{tm}\\] = Generated number of twitter mentions from regression equations.\n\\[bs\\] = Generated number of book sales from regression equations.\n\\[CE\\] = Causal Estimate from the experiment\nThe terms \\[B_0\\], \\[B_1\\], \\[B_2\\], \\[B_3\\], \\[B_4\\] and \\[e\\] are regression co-effecients and error terms.\n\nNow let us see how a regression equation for the first figure will look like.\n\\[\nbs_{orig} = B_0 + B_1(tm) + e\n\\]\nNow as author tweeting is going to be our instrumental variable let us frame a different regression equation.\n\\[\ntm = B_1 + B_2(at)+e\n\\]\n\\[B_o\\], \\[B_1\\] and \\[e\\] is used through a method of least squares to generate \\(\\hat{tm}\\). This is different from \\(tm\\) as this has been generated specifically using the variable of author tweeting excerpts. This \\(\\hat{tm}\\) will now be used in anothe regression equation to generate the new figure for book sales.\n\\[\nbs = B_3 + B_4(\\hat{tm}) + e\n\\]\n\\[\nCE = bs - bs_{orig}\n\\]\n\nThe verdict?\n\n\nIf you have a scenario like the ones we presented and there are naturally occurring experiments like academic grants for labs above a certain threshold and author tweeting excerpts from their books, do go for methodologies that leverage these occurrences. This will always give us a better edge and in the end present a better rounded picture of cause and effect.\n\n\n\n\nObservational Data\nNow comes the hard part. You can‚Äôt intervene and create randomized experiments. You can‚Äôt find occurences in your data that qualify as natural experiments. All you are left with cold hard observational data from 2 years back. This is the most explored and most complex part of causal analysis.\nTLDR;\nYou are doomed.\nLargely speaking to estimate causality from observational data only there are 3 major steps.\n\nAssume a graphical model\nMake stratifications (different groups)\nCompare the treatment across stratifications\n\nHow do we do this? Again we have 2 strategies that are quite failproof and widely used.\n\nA lot of these methods are more theoretical, which means you‚Äôll probably have to spend more time with a notebook figuring out all the variables before jumping into code/math.\n\n\nBackdoor Criterion\nBlock all the backdoor paths i.e.¬†block all the non-causal associations and study the final outcome based on the treatment. So what is a backdoor criterion again?\nA set of variables W satisfies the backdoor criteria if the following are true:\n\nW blocks all the backdoor paths from treatment (t) to outcome (Y)\nW does not contain any descendants of treatment (t).\n\nWait, so what is a backdoor?\nWell there are a lot of mathematical jargon available to define it but the general thumb-rule is\n\nArrows pointing away from treatment (t) are front doors.\nArrows pointing towards treatment (t) are backdoors.\n\n Graph with only frontdoor\n Graph with both front door and backdoor\nNow if we simply hold W or any other variable like W which is on a backdoor path as constant we will effectively close that backdoor path. Allowing us to measure the effect of treatment (t) on the outcome (Y).\n\nTest 1:\n\n\nLet‚Äôs try a little test of knowledge shall we? Given the graphical model down below, explain how you‚Äôll use Backdoor Criterion to estimate causal effect. You can write your explanation in the comments.\n\n\n\n\ngraph3bd\n\n\n\n\nPropensity Score Matching\nThis is another strategy that proves really useful when finding out causal effects from observational data. The steps to perform Propensity Score Matching are a bit complicated, but let‚Äôs break them down into chunks. We will also use an example to better understand the strategy.\nLet us suppose we are a multi-national bank and we are trying to find out if winning a lottery causes people to invest more into stocks.\n\n\n\ngraph4fd\n\n\n\nFirst, we find the propensity score i.e.¬†the probability/likelihood that the individual receives a certain treatment. In the above example, this will be the probability that a certain individual wins the lottery.\nMatch the individuals that have matching propensity scores. If we have a matching probability score of two individuals then we match them together. Nearest neighbor matching and greedy optimal matching are some of the techniques.\nVerify the quality of matches. So if two individuals having a similar chance of winning a lottery is matched, we verify the matching using statistical methods. These methods are t-test, standardized bias and can also be done using graphical representations.\nOutcome Analysis. Check how many of those matches(groups) won the lottery. In this step we try to find the mean outcome across matches and study the trend. This gives us an estimate of the causal effect over a number of groups.\n\n\nThe verdict?\n\n\nThis is the toughest of the lot. Remember causal connections are heavily dependent on counterfactual thinking but shaping existing data and conditioning on particular parameters and variables to prove a hypotheses can be a tiresome and sometimes thankless job.\n\n\n\n\nFinal Thoughts\nI realize this was quite bigger than I promised. My hope was to provide a sense of the various methodologies and examples in one place so that it can be used as a reference in times of need. From the next blog onwards we will focus on each of these segments and try to look at some examples and notebooks (python codes) to understand causality. If you have any constructive feedback do let me know in the comments.\n\n\nReferences\n\nCausal Inference in Online Systems by Amit Sharma\nAn awesome playlist for Causal Learning"
  },
  {
    "objectID": "posts/2022-01-18-what-is-computational-photography.html",
    "href": "posts/2022-01-18-what-is-computational-photography.html",
    "title": "What is Computational Photography",
    "section": "",
    "text": "Computational photography refers to digital image capture and processing techniques that use digital computation instead of optical processes. - Wikipedia\n\n\n\n\nSource: Apple Computational Photography\n\n\nThat is a neat definition, but what does it mean?\n‚ÄúIf we take a picture on instagram and apply a filter on it, is that computational photography?‚Äù\n‚ÄúWhat if we digitally enhance our photographs after we take them?‚Äù\n‚ÄúWhat about instagram filters?‚Äù\n\nTL;DR\nIn its simplest form computational photography means anything that leverages the power of computer vision and image processing to artificially enhance photographs. This could be anything from a snapchat filter to an extremely sophisticated piece of code stitching together the first ever image of a black hole.\nBut before we fully understand what computational photography means let us go through a brief lesson on photography.\n\n\nA primer on Photography\nIn simple terms we have a device which creates images by recording light reflected from real world objects on a sensor.\nThis means a camera essentially records light. Every little detail in a camera is to optimize that goal perfectly.\nSo if we zoom out a little and see the big picture, there are three main components to photography.\n\nHow much light is entering the device? (The aperture)\nHow long we are allowing the light to enter? (The shutter speed)\nHow sensitive is the sensor to the light? (ISO)\n\nIf you are already familiar with these terms, and have some experience in creating images with various combinations of these parameters feel free to skip to the next section.\nAnd now, if you are still here, let us get started with the basic components of a camera.\n\nAperture\nFirst we talk about Aperture. Now before we go all technical I want you to imagine something for me.\nFor a second, imagine that you are leaving a movie theatre. As you walk out, you are suddenly blinded by a flash of white light. You squeeze your eyes shut. It takes a couple of seconds for your eyes to readjust themselves to the brightness, and you slowly open them up again.\nSounds familiar?\nThis is exactly what aperture means.\nIt is the measure of how much the lens is opened. It is usually measured in f-stops and expressed in numbers such as 1.4,1.8,2,2.8,5.6,8,11,16.\nSomething to remember here is that the lower the number is the bigger the opening of the lens. The higher the f-stop is the smaller the opening of the lens.\n\nBut what effect does this have on the image?\n\nWell if you have a bigger opening you have more light or more information coming through. So a smaller aperture or larger f-stop would lead to darker pictures and vice versa.\n\n\n\nSource: A Beginners Guide to Photography\n\n\n\nThat‚Äôs great, what else can we do using aperture?\n\n\nThe Aperture also controls Depth-Of-Field.\n\n\nWhat is Depth-Of-Field?\n\nDepth-of-Field is simply the distance between the nearest and the farthest object in the camera‚Äôs viewing field which are in acceptable focus.\nBy acceptable focus we mean that we can make out the details of the object.\n\nBut how is this connected to aperture?\n\nWell, the bigger the aperture the more light is allowed into the sensor creating a shallow depth of field for the background and vice versa.\n\n\n\nSource: What Is F-Stop & How to Use It for Photography\n\n\n\n\nShutter Speed\nShutter speed is exactly what the name says. It is the speed with which the camera goes khiichiiik. This is usually measured in a fraction of a second for example, 1/10 th of a second. We can see numbers like 1/10,1/20,1/50,1/100,‚Ä¶,1/640.\nSo before I bring in the diagrams and the math, let me ask you a simple question-\n\nWhat happens if we take a low shutter speed like 1/10?\n\nThe camera sensor stays open for a longer period of time right? This means we are allowing more light to enter. Our image will be brighter.\nSimilarly if we have a faster shutter speed, we will not give much time for the senor to be exposed to light. This means our image would be eventually darker.\n\n\n\nSource: Shutter Speed: Everything You Need To Know\n\n\n\nThis is great? Anything else?\n\nAbsolutely! We want to capture a high speed race car (in all its glorious details) - we would need a faster shutter speed.\nNow let us imagine that we want to capture a beautiful star trail from the Himalayas. We need to capture the movement of the stars, so we would need a low shutter speed.\n\n\nISO\n\nWait, didn‚Äôt we cover everything? What is ISO?\n\nISO stands for International Organization for Standardization and it stands for sensitivity to light.\nBefore we go into ISO for digital cameras , let us revisit the old days.\n\n\n\nSource: Vintage Yashica 635\n\n\nIn the film camera ISO or ASA as it was known in those days defined the sensitivity of the film‚Äôs material to light. This is usually expressed in numbers such as 100,200,400,..1600. Now the lower the number is the less sensitive it is to light.\nWhich means in a low light environment the image would not be well exposed. Similarly if the ISO is higher, then in a low light environment it would still produce well exposed images.\n\n\n\nSource: ISO Exposure Exercise\n\n\nHowever for digital cameras ISO works differently. An image sensor is fundamentally different from a film one. Here the image is capture at a base ISO (100) always. When we select an ISO of 3200 a gain of 32X is applied to the already captured image.\nThe better the sensor the higher the tolerance of the gain.This means a good sesnor will be able to boost the image significantly without any noise, whereas a poor sensor will show noise in the image at lower values.\n\nNote: The ISO in a digital camera is applying computational photography. The sensor and chip applies gains to the image after it is captured.\n\nAperture, shutter-speed and ISO are three pillars of photography. Together they control the amount of light entering the device. Here is an image commonly known as the exposure triangle that helps us understand the relationship between these three variables.\n\n\n\nSource: The Three Elements of The Exposure Triangle\n\n\nA good photographer knows this by heart, a great photographer controls itüòé.\n\n\n\nThe Problems\n\nBut why do we need Computational Photography?\n\nEven the best of cameras have limitations. Some of them might be:\n\nCreating a small aperture is costly and time consuming. However images for constricted aperture is more aesthetic. This presents a challenge: to achieve the same quality of images using larger apertures.\nA small shutter speed for e.g 1/10 or 1/5 is realistically impossible for the human hand to keep still. Thus it is very difficult to take photographs for longer durations without a camera stand.\nNot all camera sensors are built the same. A camera sensor that will allow a high ISO value without passing any noise in the output would be immensely costly. Thus low light or night photography is completely trash in low end cameras.\n\nWith the dawn of the 21st Century science started challenging barriers and limitations. What was previously considered impossible is now one innovation away.\n\nComputational Photography aims to solve these challenges leveraging the minimal hardware requirements possible.\n\nIn the next blogpost we will look at some methods adopted by smartphones and industry giants like Google and Apple to cater Computational Photography at scale.\n\n\nLooking Back\nIn many ways photography began as an art form. The perfect instrument to capture moments. When it all began, photographs were taken and developed with care and attention in a dark room. The photographer worked hours to develop the photograph with only his memory as reference.\nWe moved from there to digital cameras that let us take photographs and visualise them instantly. The skill and dexterity of the photographer faded a little. He didn‚Äôt need a masking tape and fine toothed brush and the right mix of chemicals to bring out details.\nAnd now we arrive at the present day, photographs are no longer taken but generated. Frames are stitched together and pixels are fused by massive algorithms to bring the best version of the photograph on screen. The phtographer no longer cares about elements of photography like aperture or shutter speed.\nMaybe this is the purpose of technology - to advance science and ease our lives in the process. But great photographs have been taken even in the harshest of limitations.\nAnd perhaps that is the meaning of art - beauty even in great adversity.\n\n\nReferences\n\nComputational Photography - Wikipedia\nWhat is Computational Photography\nDefinition of ISO\nHistory of ISO\nMathemetical Expression of Noise"
  },
  {
    "objectID": "posts/2025-12-11-gwo-playground.html",
    "href": "posts/2025-12-11-gwo-playground.html",
    "title": "Interactive Grey Wolf Optimization",
    "section": "",
    "text": "This is an interactive simulation of the Grey Wolf Optimizer (GWO) algorithm attempting to solve the Rastrigin Function. The global optimum is at the center (0,0).\n\n\n\n  \n    \n    \n      \n        Population Size 30\n        \n      \n      \n      \n        Max Iterations 500\n        \n      \n      \n      \n        Speed (FPS) 30\n        \n      \n\n      Restart Simulation\n\n      \n        Best Fitness: Infinity\n        Iteration: 0\n      \n\n      \n        Alpha (Best)\n        Beta (2nd)\n        Delta (3rd)\n        Omega (Pack)"
  },
  {
    "objectID": "posts/2026-02-22-can-llms-survive-in-the-wild.html",
    "href": "posts/2026-02-22-can-llms-survive-in-the-wild.html",
    "title": "Can LLMs Survive in the Wild?",
    "section": "",
    "text": "What happens when you drop a language model into a hostile procedurally generated world and ask it to not die? Turns out, survival is a remarkably good test of intelligence."
  },
  {
    "objectID": "posts/2026-02-22-can-llms-survive-in-the-wild.html#the-problem-with-benchmarks",
    "href": "posts/2026-02-22-can-llms-survive-in-the-wild.html#the-problem-with-benchmarks",
    "title": "Can LLMs Survive in the Wild?",
    "section": "The Problem with Benchmarks",
    "text": "The Problem with Benchmarks\nWe keep testing LLMs the way we test students: multiple choice questions, coding puzzles, and math problems, and whatever the hot fudge humanity‚Äôs last exam is. But intelligence as it has evolved in the real world is not about answering questions, it is about staying alive.\nIndeed the real test of any intellegence is just finding a way to stay alive.\n\n\n\nAs a wise man once said.\n\n\nAn organism in the wild must continuously perceive threats, manage scarce resources, plan escape routes, and pursue long-term goals‚Äîall at the same time. There is no ‚Äúcorrect answer‚Äù to look up. There is only the next decision, and if you get it wrong, you are dead.\nThat is the core idea behind Earth-33: a 2D survival simulation where agents: LLMs, hand-crafted heuristics, and bio-inspired swarm algorithms, are dropped into procedurally generated worlds and evaluated on a single, unforgiving metric: how long can you stay alive?"
  },
  {
    "objectID": "posts/2026-02-22-can-llms-survive-in-the-wild.html#what-is-earth-33",
    "href": "posts/2026-02-22-can-llms-survive-in-the-wild.html#what-is-earth-33",
    "title": "Can LLMs Survive in the Wild?",
    "section": "What Is Earth-33?",
    "text": "What Is Earth-33?\nEarth-33 is a simulation framework built in Python that models realistic survival pressure across multiple interconnected systems:\n\nProcedural terrain with 7 biome types (plains, forest, desert, tundra, mountain, water, swamp)\nDynamic climate with seasonal and diurnal temperature cycles, elevation-based lapse rates\nOrganism physiology tracking hydration, energy, core temperature, fatigue, injury, and infection\nHunter NPCs that patrol the map with hidden detection radii, chase on sight, and kill on contact\nA hidden trophy that the agent must find using graduated proximity hints (warmer/colder, directional cues)\nFog of war that limits the agent‚Äôs perception to a small radius around its position\n\nThe agent receives a JSON observation each turn‚Äîvitals, nearby terrain, visible hunters, trophy hints‚Äîand must respond with one of 10 actions: move in four cardinal directions, rest, drink, forage, build shelter, hide, or signal.\nEvery decision has a physiological cost. Moving drains energy and hydration. Resting recovers fatigue but burns calories. Staying in the desert without water is a death sentence. Wandering into a hunter‚Äôs detection radius triggers a chase you probably will not survive.\nThe simulation runs at 1 simulated hour per step, for up to 800 steps on the hardest scenario. That is roughly 33 days of in-game survival time. No agent has survived all 800 steps yet."
  },
  {
    "objectID": "posts/2026-02-22-can-llms-survive-in-the-wild.html#why-use-this-to-test-llms",
    "href": "posts/2026-02-22-can-llms-survive-in-the-wild.html#why-use-this-to-test-llms",
    "title": "Can LLMs Survive in the Wild?",
    "section": "Why Use This to Test LLMs?",
    "text": "Why Use This to Test LLMs?\nTraditional LLM benchmarks measure mostly either knowledge retrieval or pattern matching or both. Earth-33 measures something fundamentally different:\n\nMulti-objective reasoning: The agent must balance six physiological needs simultaneously while avoiding hunters and pursuing the trophy. There is no single ‚Äúright‚Äù answer,it must balance trade-offs.\nTemporal planning: Drinking water now means you cannot forage. Resting recovers fatigue but wastes precious daylight. The agent must reason about consequences several steps ahead. This is a toned down version of how survival works.\nAdversarial adaptation: Hunter NPCs have hidden detection radii. The LLM must infer these radii from partial observations, seeing a hunter that is not chasing implies the radius is smaller than the current distance. This tests genuine theory-of-mind reasoning.\nGraceful degradation: When things go wrong (and they always do), can the agent recover? A near-death dehydration event followed by a hunter sighting tests whether the LLM organism can reprioritize under pressure.\nGrounded decision-making: Unlike chatbot evaluations, every action has irreversible physical consequences. You cannot ‚Äúundo‚Äù walking into a swamp."
  },
  {
    "objectID": "posts/2026-02-22-can-llms-survive-in-the-wild.html#the-agents",
    "href": "posts/2026-02-22-can-llms-survive-in-the-wild.html#the-agents",
    "title": "Can LLMs Survive in the Wild?",
    "section": "The Agents",
    "text": "The Agents\nEarth-33 POC ships with 7 agent types spanning three paradigms. Here is what each one is, what it tests, and how it performs.\n\nThe Heuristic Agent (Hand-Crafted Custom Rules)\nThe heuristic agent is a 12-tier priority engine. It checks conditions in order: critical dehydration first, then critical energy, temperature danger, fatigue, hunter proximity, and so on‚Äîand fires the first rule that matches.\nWhat it tests: This is the ceiling for hand-crafted intelligence. It represents what a careful human programmer can achieve by encoding survival knowledge as if-then rules. It is the baseline that every other agent must beat to justify its complexity.\nExample behavior: If hydration drops below 25% and water is available, it drinks immediately. If a hunter is within 6 cells, it flees. If no threats are present and a trophy hint points north, it moves north.\n\n\n\nThe heuristic agent navigating the hunt scenario. Notice the systematic resource management‚Äîit stays near water sources and avoids open terrain.\n\n\n\n\n\nThe LLM Agent (Gemini 2.0 Flash / GPT-4o-mini/ Custom LLM)\nThe LLM agent receives the full JSON observation and a detailed system prompt that instructs it to follow a 5-step planning protocol: assess vitals, analyze hunters, plan a path, consider the trophy, and decide. It outputs a structured JSON action.\nThe agent uses a provider fallback chain: it first tries Gemini 2.0 Flash, falls back to GPT-4o-mini if the Gemini quota is exhausted, and ultimately falls back to the heuristic agent if both APIs fail. We can think of extending this to custom LLM backends for vLLM.\nWhat it tests: This is the real test. Can an LLM, given nothing but text observations and a planning prompt, reason about multi-dimensional survival trade-offs? Can it infer hidden hunter radii from partial information? Can it balance short-term survival against long-term trophy pursuit?\nExample behavior: The LLM might reason: ‚ÄúHydration is at 40%, but there is a hunter 8 cells to the east. The hunter was not chasing at distance 10 last turn, so its radius is likely less than 10. I will move south toward the water source, keeping at least 12 cells from the hunter. Trophy direction is southeast, so this also makes progress toward the objective.‚Äù\n\n\n\nGPT-4o-mini navigating the hunt scenario. Watch how it reasons through trade-offs‚Äîsometimes brilliantly, sometimes fatally.\n\n\n\n\n\nThe Random Agent (Baseline)\nThe random agent selects uniformly from valid actions each turn. No intelligence, no strategy.\nWhat it tests: Nothing. This is the floor. Any agent that cannot consistently outperform random action selection has no business calling itself intelligent (in the world of Earth-33).\n\n\n\nThe random agent wandering aimlessly. Note the erratic path and rapid vital depletion‚Äîit has no concept of resource management.\n\n\n\n\n\nParticle Swarm Optimization (PSO)\nPSO maintains a swarm of 10 virtual particles that explore the grid around the organism. Each particle tracks its personal best position and shares a global best. The organism moves toward the global best‚Äîthe most promising location found by any particle.\nWhat it tests: Can distributed parallel search outperform sequential reasoning? PSO excels at exploration‚Äîit covers more ground than any other agent‚Äîbut it has no concept of survival. When vitals drop critically, it delegates to the heuristic agent as a survival override.\nExample behavior: 10 particles fan out in a 15-cell radius, evaluating positions based on a fitness function that balances trophy attraction, hunter avoidance, resource proximity, and terrain cost. The organism follows the best particle.\n\n\n\nPSO agent with its particle swarm visible as faded dots. The cyan circle shows the search radius. Watch how the particles converge on promising regions.\n\n\n\n\n\nGrey Wolf Optimization (GWO)\nGWO simulates a wolf pack hierarchy: alpha (best solution), beta (second best), delta (third), and omega (the rest). Each wolf moves toward a weighted average of the top three positions. The parameter a decays from 2 to 0 over the episode, shifting from exploration to exploitation.\nWhat it tests: Does hierarchical decision-making help in survival scenarios? GWO‚Äôs pack structure means it converges faster than PSO but explores less. It tests whether leadership, following the best-known solution, is a viable survival strategy.\nExample behavior: 8 wolves explore the terrain. The alpha wolf finds a position near water with low hunter risk. The entire pack gradually converges on that region, tightening the search as the episode progresses.\n\n\n\nGWO agent with its wolf pack. The alpha-beta-delta hierarchy drives convergence‚Äîwatch the pack tighten around the best-known position.\n\n\n\n\n\nWhale Optimization Algorithm (WOA)\nWOA models humpback whale bubble-net hunting with two mechanisms: shrinking encirclement (tightening a circle around prey) and spiral bubble-net attack (logarithmic spiral approach). Each whale randomly uses one mechanism per step.\nWhat it tests: Can a biomimetic search strategy that was designed for continuous optimization work in a discrete survival grid? WOA‚Äôs spiral mechanism provides unique movement patterns that other agents cannot produce.\nExample behavior: 6 whales alternate between circling the best-known position and spiraling toward it. When the exploration parameter is high, whales move toward random peers, maintaining diversity.\n\n\n\nWOA agent using bubble-net hunting. The dual spiral-and-encircle mechanism creates distinctive search patterns.\n\n\n\n\n\nAnt Colony Optimization (ACO)\nACO deploys 12 virtual ants that walk probabilistic paths from the organism‚Äôs position, guided by a pheromone grid. Good paths get reinforced; pheromone evaporates over time. The organism follows the best ant‚Äôs trail.\nWhat it tests: Can stigmergic communication‚Äîindirect coordination through environmental markers‚Äîproduce intelligent survival behavior? ACO is the only agent that builds a persistent memory of the environment through its pheromone map.\nExample behavior: 12 ants walk 30-50 steps each, choosing neighbors probabilistically based on pheromone intensity and a heuristic score. The best ant‚Äôs path is reinforced, creating trails toward resources and away from dangers.\n\n\n\nACO agent with its ant trails. The pheromone-guided pathfinding creates persistent exploration memory across steps."
  },
  {
    "objectID": "posts/2026-02-22-can-llms-survive-in-the-wild.html#running-the-simulations",
    "href": "posts/2026-02-22-can-llms-survive-in-the-wild.html#running-the-simulations",
    "title": "Can LLMs Survive in the Wild?",
    "section": "Running the Simulations",
    "text": "Running the Simulations\nAll simulations are reproducible and seeded. Here are the commands to replicate every result in this post.\n\nSetup\ncd earth_33\npip install -e .\n\n\nSingle Episode (Visual Mode)\n# Heuristic agent on the hardest scenario\npython -m cli run_episode --config configs/hunt.yaml --agent heuristic --seed 42\n\n# LLM agent (requires GEMINI_API_KEY or OPENAI_API_KEY in .env)\npython -m cli run_episode --config configs/hunt.yaml --agent llm --seed 42\n\n# Swarm agents\npython -m cli run_episode --config configs/hunt.yaml --agent pso --seed 42\npython -m cli run_episode --config configs/hunt.yaml --agent gwo --seed 42\npython -m cli run_episode --config configs/hunt.yaml --agent woa --seed 42\npython -m cli run_episode --config configs/hunt.yaml --agent aco --seed 42\n\n\nRecord a GIF\npython -m cli run_episode --config configs/hunt.yaml --agent pso --seed 42 --record runs/pso_demo.gif\n\n\nBatch Evaluation (Headless)\n# Evaluate across 10 seeds\npython -m cli evaluate --config configs/hunt.yaml --agent heuristic --seeds 0-9 --output runs/eval_heuristic\npython -m cli evaluate --config configs/hunt.yaml --agent llm --seeds 0-9 --output runs/eval_llm\npython -m cli evaluate --config configs/hunt.yaml --agent pso --seeds 0-9 --output runs/eval_pso\npython -m cli evaluate --config configs/hunt.yaml --agent gwo --seeds 0-9 --output runs/eval_gwo\npython -m cli evaluate --config configs/hunt.yaml --agent woa --seeds 0-9 --output runs/eval_woa\npython -m cli evaluate --config configs/hunt.yaml --agent aco --seeds 0-9 --output runs/eval_aco\n\n# Cross-environment evaluation\nfor env in forest desert tundra; do\n  for agent in heuristic llm pso aco; do\n    python -m cli evaluate --config configs/${env}.yaml --agent ${agent} --seeds 0-9 --output runs/eval_${env}_${agent}\n  done\ndone\n\n\nOther Scenarios\n# Easier environments\npython -m cli run_episode --config configs/forest.yaml --agent heuristic --seed 42\npython -m cli run_episode --config configs/desert.yaml --agent pso --seed 42\npython -m cli run_episode --config configs/tundra.yaml --agent gwo --seed 42"
  },
  {
    "objectID": "posts/2026-02-22-can-llms-survive-in-the-wild.html#the-mathematics-of-survival",
    "href": "posts/2026-02-22-can-llms-survive-in-the-wild.html#the-mathematics-of-survival",
    "title": "Can LLMs Survive in the Wild?",
    "section": "The Mathematics of Survival",
    "text": "The Mathematics of Survival\nBeneath the pixel art and GIFs, Earth-33 is a coupled dynamical system. Every step of the simulation computes interactions between physiology, climate, terrain, and agent policy. This section lays out the mathematics that govern the world and the three paradigms agents use to navigate it.\n\nThe Organism as a Dynamical System\nThe organism‚Äôs state at time \\(t\\) is a 6-dimensional vector:\n\\[\n\\mathbf{s}_t = \\begin{pmatrix} h_t \\\\ e_t \\\\ \\theta_t \\\\ f_t \\\\ \\iota_t \\\\ \\phi_t \\end{pmatrix} \\in [0, 100]^4 \\times \\mathbb{R} \\times [0, 100]^2\n\\]\nwhere \\(h\\) is hydration, \\(e\\) is energy, \\(\\theta\\) is core body temperature (in ¬∞C, nominally 37), \\(f\\) is fatigue, \\(\\iota\\) is injury, and \\(\\phi\\) is infection. Each step, the simulation applies a state transition:\n\\[\n\\mathbf{s}_{t+1} = \\mathbf{s}_t + \\Delta \\mathbf{s}(a_t, \\mathbf{c}_t) \\cdot \\Delta t\n\\]\nwhere \\(a_t \\in \\mathcal{A}\\) is the chosen action, \\(\\mathbf{c}_t\\) is the environmental context (terrain, climate, wildlife), and \\(\\Delta t = 1\\) hour.\n\n\nPhysiological Drain Equations\nEvery action carries a metabolic multiplier \\(\\mu(a)\\):\n\nActivity multipliers. Movement is further scaled by terrain cost \\(c_\\text{terrain}\\).\n\n\nAction\n\\(\\mu(a)\\)\n\n\n\n\nMove (N/S/E/W)\n\\(1.5 \\times c_\\text{terrain}\\)\n\n\nForage\n1.3\n\n\nBuild Shelter\n1.4\n\n\nDrink\n0.9\n\n\nHide / Signal\n0.8\n\n\nRest\n0.7\n\n\n\nThe terrain movement cost \\(c_\\text{terrain}\\) varies by biome:\n\\[\nc_\\text{terrain} \\in \\{1.0_\\text{plains},\\; 1.2_\\text{tundra},\\; 1.3_\\text{forest},\\; 1.4_\\text{desert},\\; 1.8_\\text{swamp},\\; 2.0_\\text{mountain},\\; 3.0_\\text{water}\\}\n\\]\nHydration drains as a function of activity and heat:\n\\[\n\\Delta h_t = -\\Big(\\beta_h \\cdot \\mu(a_t) + \\underbrace{\\max\\!\\big(0,\\; \\tfrac{T_\\text{air} - 30}{10}\\big) \\cdot 0.8}_{\\text{heat stress}}\\Big) \\cdot d \\cdot \\Delta t\n\\]\nwhere \\(\\beta_h = 0.8\\) is the base drain rate and \\(d\\) is the scenario difficulty multiplier.\nEnergy drains similarly but without the heat term:\n\\[\n\\Delta e_t = -\\beta_e \\cdot \\mu(a_t) \\cdot d \\cdot \\Delta t, \\quad \\beta_e = 0.6\n\\]\nCore temperature drifts toward ambient air temperature via Newton‚Äôs law of cooling:\n\\[\n\\Delta \\theta_t = \\kappa \\cdot (T_\\text{air} - \\theta_t) \\cdot \\Delta t, \\quad \\kappa = \\begin{cases} 0.008 \\times 0.3 & \\text{if shelter active} \\\\ 0.008 & \\text{otherwise} \\end{cases}\n\\]\nShelter reduces thermal coupling by 70%, which is why it is the difference between life and death in the tundra.\nFatigue accumulates during activity and recovers during rest:\n\\[\n\\Delta f_t = \\begin{cases} -4.0 \\cdot \\Delta t & \\text{if } a_t = \\texttt{REST} \\\\ 0.7 \\cdot \\mu(a_t) \\cdot d \\cdot \\Delta t & \\text{otherwise} \\end{cases}\n\\]\n\n\nDeath Conditions\nThe organism dies instantly when any vital crosses its threshold:\n\\[\n\\text{DEAD} \\iff h \\leq 0 \\;\\lor\\; e \\leq 0 \\;\\lor\\; \\theta \\leq 30 \\;\\lor\\; \\theta \\geq 42 \\;\\lor\\; \\iota \\geq 100 \\;\\lor\\; \\phi \\geq 100\n\\]\nThis creates a multi-constraint feasibility problem: the agent must keep all six vitals within bounds simultaneously. A single lapse in any dimension is fatal.\n\n\nClimate Model\nAir temperature at position \\((x, y)\\) and time step \\(t\\) is computed as:\n\\[\nT_\\text{air}(x, y, t) = T_\\text{base}(b) - \\lambda \\cdot \\frac{\\text{elev}(x,y)}{1000} + A_s(b) \\cdot \\xi \\cdot \\sin\\!\\Big(\\frac{2\\pi \\cdot \\text{day}}{365}\\Big) + A_d(b) \\cdot \\xi \\cdot \\sin\\!\\Big(\\frac{2\\pi \\cdot \\text{hour}}{24} - \\frac{\\pi}{2}\\Big) + \\varepsilon\n\\]\nwhere \\(T_\\text{base}(b)\\) is the biome base temperature (e.g., \\(-5¬∞C\\) for tundra, \\(30¬∞C\\) for desert), \\(\\lambda = 6.5\\) ¬∞C/km is the atmospheric lapse rate, \\(A_s\\) and \\(A_d\\) are seasonal and diurnal amplitudes, \\(\\xi\\) is the temperature extremity multiplier, and \\(\\varepsilon \\sim \\mathcal{N}(0, 1)\\) is stochastic noise.\nThe seasonal and diurnal amplitudes create temperature swings that vary by biome. In the desert, diurnal amplitude is \\(\\pm 10¬∞C\\), making nighttime cold a real danger even in a ‚Äúhot‚Äù biome.\n\n\nThe Survival Optimization Problem\nFormally, we can define the survival problem as:\n\\[\n\\max_{\\pi} \\;\\; \\mathbb{E}\\!\\Big[\\sum_{t=0}^{T_\\text{max}} \\mathbb{1}[\\text{alive at } t]\\Big] \\quad \\text{subject to} \\quad \\mathbf{s}_t \\in \\mathcal{S}_\\text{viable} \\;\\;\\forall\\, t\n\\]\nwhere \\(\\pi: \\mathcal{O} \\to \\mathcal{A}\\) is the agent‚Äôs policy mapping observations to actions, \\(T_\\text{max} = 800\\) steps, and \\(\\mathcal{S}_\\text{viable}\\) is the set of non-lethal states. Each agent paradigm attacks this problem differently.\n\n\n\nHow the Heuristic Agent Solves It: Priority Cascades\nThe heuristic agent implements \\(\\pi\\) as a fixed priority cascade. It evaluates conditions top-to-bottom and fires the first matching rule:\n\n\n\n\n\n\n\nheuristic\n\n\n\nstart\n\nObserve State\n\n\n\np1\n\nh &lt; 25 ‚àß water &gt; 0.1?\n\n\n\nstart-&gt;p1\n\n\n\n\n\na1\n\nDRINK\n\n\n\np1-&gt;a1\n\n\nyes\n\n\n\np2\n\ne &lt; 25 ‚àß veg &gt; 0.1?\n\n\n\np1-&gt;p2\n\n\nno\n\n\n\na2\n\nFORAGE\n\n\n\np2-&gt;a2\n\n\nyes\n\n\n\np3\n\nŒ∏_air &gt; 35 ‚à® Œ∏_air &lt; 5?\n\n\n\np2-&gt;p3\n\n\nno\n\n\n\na3\n\nBUILD SHELTER / REST\n\n\n\np3-&gt;a3\n\n\nyes\n\n\n\np4\n\nf &gt; 75?\n\n\n\np3-&gt;p4\n\n\nno\n\n\n\na4\n\nREST\n\n\n\np4-&gt;a4\n\n\nyes\n\n\n\np5\n\nhunter dist ‚â§ 6?\n\n\n\np4-&gt;p5\n\n\nno\n\n\n\na5\n\nFLEE (opposite dir)\n\n\n\np5-&gt;a5\n\n\nyes\n\n\n\np6\n\nh &lt; 50?\n\n\n\np5-&gt;p6\n\n\nno\n\n\n\na6\n\nMove ‚Üí water\n\n\n\np6-&gt;a6\n\n\nyes\n\n\n\np7\n\ne &lt; 50?\n\n\n\np6-&gt;p7\n\n\nno\n\n\n\na7\n\nFORAGE / Move ‚Üí veg\n\n\n\np7-&gt;a7\n\n\nyes\n\n\n\np8\n\ntrophy hint?\n\n\n\np7-&gt;p8\n\n\nno\n\n\n\na8\n\nMove ‚Üí trophy dir\n\n\n\np8-&gt;a8\n\n\nyes\n\n\n\np9\n\ndefault\n\n\n\np8-&gt;p9\n\n\nno\n\n\n\na9\n\nREST\n\n\n\np9-&gt;a9\n\n\n\n\n\n\n The heuristic agent‚Äôs 12-tier priority cascade. Evaluation proceeds top-to-bottom; the first matching rule fires. \n\n\n\nThe key insight is that survival conditions always dominate trophy pursuit. The agent never chases the trophy unless all six vitals are in safe ranges. This is why it survives longest but never finds the trophy: it is a conservative policy that maximizes \\(\\min_i(v_i)\\) rather than expected reward.\n\n\n\nHow the Swarm Agents Solve It: Fitness Landscapes\nThe swarm agents reframe survival as a continuous optimization problem. They deploy virtual populations (particles, wolves, whales, ants) across the grid, each evaluating a shared fitness function \\(F(x, y)\\) that encodes survival priorities:\n\\[\nF(x, y) = \\underbrace{2.0 \\cdot (\\mathbf{d} \\cdot \\hat{\\mathbf{t}}) \\cdot m_t}_{\\text{trophy attraction}} - \\underbrace{1.5 \\sum_{j} P_j(x, y)}_{\\text{hunter avoidance}} + \\underbrace{5.0 \\cdot w(x,y) \\cdot u_h}_{\\text{water urgency}} + \\underbrace{4.0 \\cdot v(x,y) \\cdot u_e}_{\\text{food urgency}} - \\underbrace{2.0 \\cdot (c_\\text{terrain} - 1)}_{\\text{terrain cost}} - \\underbrace{3.0 \\cdot r(x,y)}_{\\text{wildlife risk}}\n\\]\nwhere:\n\n\\(\\mathbf{d} = (x - x_\\text{org},\\; y - y_\\text{org})\\) is the displacement from the organism, \\(\\hat{\\mathbf{t}}\\) is the estimated trophy direction, and \\(m_t\\) is the trophy magnitude (updated by warmer/colder feedback)\n\\(P_j(x,y) = \\begin{cases} (R_j - d_j)^2 \\cdot \\gamma_j & \\text{if } d_j &lt; R_j \\\\ 0 & \\text{otherwise} \\end{cases}\\) is the hunter penalty, where \\(R_j = 7\\) is the estimated detection radius, \\(d_j\\) is the distance to hunter \\(j\\), and \\(\\gamma_j = 2\\) if the hunter is actively chasing\n\\(u_h = \\max(0, \\frac{60 - h}{60})\\) and \\(u_e = \\max(0, \\frac{60 - e}{60})\\) are urgency factors that amplify resource attraction as vitals drop\n\\(w(x,y)\\), \\(v(x,y)\\), and \\(r(x,y)\\) are the water availability, vegetation biomass, and wildlife risk at position \\((x,y)\\)\n\nAll swarm agents share a survival override: when any vital enters a critical zone (\\(h &lt; 20\\), \\(e &lt; 20\\), \\(f &gt; 85\\), \\(\\iota &gt; 70\\), or a hunter within 3 cells), the swarm is bypassed and the heuristic agent takes control. This creates a hybrid architecture: swarm for exploration, heuristics for survival.\n\nPSO: Velocity-Position Updates\nEach particle \\(i\\) maintains a position \\(\\mathbf{x}_i\\), velocity \\(\\mathbf{v}_i\\), and personal best \\(\\mathbf{p}_i\\). The swarm shares a global best \\(\\mathbf{g}\\):\n\\[\n\\mathbf{v}_i^{(k+1)} = \\underbrace{w \\cdot \\mathbf{v}_i^{(k)}}_{\\text{inertia}} + \\underbrace{c_1 \\cdot r_1 \\cdot (\\mathbf{p}_i - \\mathbf{x}_i^{(k)})}_{\\text{cognitive}} + \\underbrace{c_2 \\cdot r_2 \\cdot (\\mathbf{g} - \\mathbf{x}_i^{(k)})}_{\\text{social}}\n\\]\n\\[\n\\mathbf{x}_i^{(k+1)} = \\mathbf{x}_i^{(k)} + \\mathbf{v}_i^{(k+1)}\n\\]\nwith inertia weight \\(w = 0.7\\), cognitive coefficient \\(c_1 = 1.5\\), social coefficient \\(c_2 = 2.0\\), and \\(r_1, r_2 \\sim U(0,1)\\). Velocities are clamped to \\(|\\mathbf{v}| \\leq 3.0\\) cells/step.\n\n\n\n\n\n\n\npso\n\n\n\npos\n\nPosition x·µ¢\n\n\n\nfit\n\nF(x, y)\n\n\n\npos-&gt;fit\n\n\nevaluate\n\n\n\nvel\n\nVelocity v·µ¢\n\n\n\nnew_vel\n\nNew Velocity\n\n\n\nvel-&gt;new_vel\n\n\n√ó w (inertia)\n\n\n\npbest\n\nPersonal Best p·µ¢\n\n\n\npbest-&gt;new_vel\n\n\n√ó c‚ÇÅr‚ÇÅ (cognitive)\n\n\n\ngbest\n\nGlobal Best g\n\n\n\ngbest-&gt;new_vel\n\n\n√ó c‚ÇÇr‚ÇÇ (social)\n\n\n\nfit-&gt;pbest\n\n\nif F &gt; F(p·µ¢)\n\n\n\nfit-&gt;gbest\n\n\nif F &gt; F(g)\n\n\n\nnew_pos\n\nNew Position\n\n\n\nnew_vel-&gt;new_pos\n\n\nx += v\n\n\n\nnew_pos-&gt;pos\n\n\nnext iter\n\n\n\n\n PSO information flow. Each particle balances three forces: momentum (inertia), memory (personal best), and communication (global best). \n\n\n\n\n\nGWO: Pack Hierarchy\nGWO ranks the population and assigns leadership roles: alpha (\\(\\alpha\\), best fitness), beta (\\(\\beta\\), second), and delta (\\(\\delta\\), third). Each wolf updates its position as a weighted average of influence from all three leaders:\n\\[\n\\mathbf{x}_i^{(k+1)} = \\frac{\\mathbf{X}_1 + \\mathbf{X}_2 + \\mathbf{X}_3}{3}\n\\]\nwhere:\n\\[\n\\mathbf{X}_1 = \\mathbf{x}_\\alpha - \\mathbf{A}_1 \\cdot |\\mathbf{C}_1 \\cdot \\mathbf{x}_\\alpha - \\mathbf{x}_i|, \\quad\n\\mathbf{X}_2 = \\mathbf{x}_\\beta - \\mathbf{A}_2 \\cdot |\\mathbf{C}_2 \\cdot \\mathbf{x}_\\beta - \\mathbf{x}_i|, \\quad\n\\mathbf{X}_3 = \\mathbf{x}_\\delta - \\mathbf{A}_3 \\cdot |\\mathbf{C}_3 \\cdot \\mathbf{x}_\\delta - \\mathbf{x}_i|\n\\]\nThe parameter \\(a\\) decays linearly from 2 to 0 over the episode:\n\\[\na = 2 \\cdot \\Big(1 - \\frac{k}{K_\\text{max}}\\Big), \\quad \\mathbf{A} = 2a \\cdot \\mathbf{r}_1 - a, \\quad \\mathbf{C} = 2 \\cdot \\mathbf{r}_2\n\\]\nWhen \\(|A| &gt; 1\\), wolves explore (diverge). When \\(|A| &lt; 1\\), they exploit (converge). This automatic exploration-exploitation balance is elegant but blind to physiological state.\n\n\nWOA: Bubble-Net Mechanics\nWOA alternates between two mechanisms with probability \\(p \\sim U(0,1)\\):\n\\[\n\\mathbf{x}_i^{(k+1)} = \\begin{cases}\n\\mathbf{x}^* - \\mathbf{A} \\cdot |\\mathbf{C} \\cdot \\mathbf{x}^* - \\mathbf{x}_i| & \\text{if } p &lt; 0.5 \\text{ and } |A| &lt; 1 \\quad \\textit{(encirclement)} \\\\[4pt]\n\\mathbf{x}_\\text{rand} - \\mathbf{A} \\cdot |\\mathbf{C} \\cdot \\mathbf{x}_\\text{rand} - \\mathbf{x}_i| & \\text{if } p &lt; 0.5 \\text{ and } |A| \\geq 1 \\quad \\textit{(exploration)} \\\\[4pt]\nD' \\cdot e^{bl} \\cdot \\cos(2\\pi l) + \\mathbf{x}^* & \\text{if } p \\geq 0.5 \\quad \\textit{(spiral)}\n\\end{cases}\n\\]\nwhere \\(D' = |\\mathbf{x}^* - \\mathbf{x}_i|\\) is the distance to the best whale, \\(b = 1.0\\) is the spiral shape constant, and \\(l \\sim U(-1, 1)\\). The spiral creates a logarithmic approach trajectory that mimics the humpback whale‚Äôs bubble-net hunting.\n\n\nACO: Pheromone-Guided Walks\nACO is the most distinct of the swarm agents. Instead of a population exploring in parallel, it uses stigmergy: indirect communication through the environment via a pheromone grid \\(\\tau(x,y)\\).\nEach ant \\(k\\) takes a random walk of up to \\(L\\) steps, choosing its next cell with probability:\n\\[\nP(x', y') = \\frac{[\\tau(x', y')]^\\alpha \\cdot [\\eta(x', y')]^\\beta}{\\sum_{(x'', y'') \\in \\mathcal{N}} [\\tau(x'', y'')]^\\alpha \\cdot [\\eta(x'', y'')]^\\beta}\n\\]\nwhere \\(\\alpha = 1.0\\) is the pheromone influence exponent, \\(\\beta = 2.0\\) is the heuristic influence exponent, \\(\\eta(x',y') = \\max(0.01, F(x',y') + 100)\\) is the heuristic desirability (shifted fitness), and \\(\\mathcal{N}\\) is the set of valid adjacent cells.\nAfter all ants walk, the best path is reinforced and all pheromone evaporates:\n\\[\n\\tau(x,y) \\leftarrow (1 - \\rho) \\cdot \\tau(x,y) + \\Delta\\tau_\\text{best}(x,y)\n\\]\nwhere \\(\\rho = 0.1\\) is the evaporation rate and \\(\\Delta\\tau_\\text{best}\\) deposits pheromone along the best ant‚Äôs path proportional to its fitness. Pheromone is clamped to \\([\\tau_\\text{min}, \\tau_\\text{max}] = [0.1, 10.0]\\).\nThis creates a persistent memory of the environment. Good paths accumulate pheromone over multiple steps, which is why ACO outperforms the other swarm agents: it learns from its history rather than restarting each step from scratch.\n\n\n\n\n\n\n\naco\n\n\n\ninit\n\nInitialize œÑ(x,y) = 1.0\n\n\n\nants\n\nN ants walk L steps\neach choosing P(x',y')\n\n\n\ninit-&gt;ants\n\n\n\n\n\neval\n\nEvaluate fitness\nof each ant path\n\n\n\nants-&gt;eval\n\n\n\n\n\nbest\n\nSelect best path\n\n\n\neval-&gt;best\n\n\n\n\n\ndeposit\n\nDeposit ŒîœÑ on\nbest path\n\n\n\nbest-&gt;deposit\n\n\n\n\n\nmove\n\nOrganism moves toward\nbest ant position\n\n\n\nbest-&gt;move\n\n\n\n\n\nevap\n\nEvaporate:\nœÑ ‚Üê (1-œÅ)¬∑œÑ\n\n\n\ndeposit-&gt;evap\n\n\n\n\n\nevap-&gt;ants\n\n\nnext iteration\n\n\n\n\n ACO‚Äôs pheromone feedback loop. Ants explore stochastically, reinforce good paths, and pheromone evaporates to prevent stagnation. \n\n\n\n\n\n\n\nHow the LLM Solves It: In-Context Reasoning\nThe LLM agent takes a fundamentally different approach. It does not optimize a fitness function or follow a priority cascade. Instead, it receives the raw observation as JSON text and must reason its way to an action.\nThe system prompt instructs a 5-step protocol:\n\n\n\n\n\n\n\nllm\n\n\n\nobs\n\nJSON Observation\nvitals, terrain,\nhunters, trophy\n\n\n\ns1\n\n1. ASSESS\nvitals & threats\n\n\n\nobs-&gt;s1\n\n\n\n\n\ns2\n\n2. ANALYZE\nhunter radii from\nobservation history\n\n\n\ns1-&gt;s2\n\n\n\n\n\ns3\n\n3. PLAN\n2-3 candidate moves\n\n\n\ns2-&gt;s3\n\n\n\n\n\ns4\n\n4. FACTOR\ntrophy direction hints\n\n\n\ns3-&gt;s4\n\n\n\n\n\ns5\n\n5. DECIDE\nsafest action toward trophy\n\n\n\ns4-&gt;s5\n\n\n\n\n\nout\n\n\n\nJSON Output\naction + reason\n+ confidence\n\n\n\ns5-&gt;out\n\n\n\n\n\n\n The LLM agent‚Äôs reasoning protocol. Each step feeds into the next, culminating in a structured JSON action. \n\n\n\nThe LLM must implicitly solve the same optimization problem the swarm agents solve explicitly, but through language. It must:\n\nParse a JSON observation and extract relevant features\nMaintain a mental model of hunter detection radii from partial observations (if the hunter was visible at distance 10 but not chasing, then \\(R_\\text{hunter} &lt; 10\\))\nWeigh multiple objectives (vitals, hunters, trophy) without an explicit fitness function\nOutput a valid action in structured JSON format\n\nThe critical difference: the swarm agents optimize over space (where should I go?), while the LLM optimizes over actions (what should I do?). The swarm agents have access to the full world grid through the snapshot and can evaluate thousands of positions. The LLM sees only a 3-cell observation radius and must reason from limited information.\nThis makes the LLM agent simultaneously more powerful (it can reason about abstract concepts like ‚Äúsave energy for later‚Äù) and more fragile (it can hallucinate actions, misparse observations, or simply forget to eat). As the results show, the fragility currently dominates."
  },
  {
    "objectID": "posts/2026-02-22-can-llms-survive-in-the-wild.html#the-evaluation-who-survives-longest",
    "href": "posts/2026-02-22-can-llms-survive-in-the-wild.html#the-evaluation-who-survives-longest",
    "title": "Can LLMs Survive in the Wild?",
    "section": "The Evaluation: Who Survives Longest?",
    "text": "The Evaluation: Who Survives Longest?\nHere are the results from batch evaluation across 10 random seeds on the hunt scenario (the hardest configuration: 6 hunters, 1.3x drain multiplier, 800 max steps). FOr the LLM part we include GPT-4o-mini as the LLM agent alongside the heuristic and swarm baselines.\n\nHunt scenario (hard mode), 10 seeds each.\n\n\n\n\n\n\n\n\n\n\nAgent\nType\nAvg Steps\nAvg Days\nTop Death Cause\nTrophy Wins\n\n\n\n\nHeuristic\nRule-based\n87.5\n3.65\nStarvation (60%)\n0\n\n\nACO\nSwarm\n60.6\n2.53\nDehydration (40%)\n0\n\n\nRandom\nBaseline\n51.8\n2.16\nDehydration (60%)\n0\n\n\nPSO\nSwarm\n42.0\n1.75\nDehydration (60%)\n0\n\n\nGWO\nSwarm\n41.3\n1.72\nStarvation (50%)\n0\n\n\nWOA\nSwarm\n40.4\n1.68\nDehydration (50%)\n0\n\n\nGPT-4o-mini\nLLM\n33.7\n1.40\nStarvation (100%)\n0\n\n\n\n\nKey Observations\nThe heuristic agent dominates survival time. At 87.5 average steps (3.65 days), it nearly doubles the swarm agents and more than doubles the LLM. This is not surprising‚Äîit was designed to survive, with hand-crafted rules that prioritize drinking when thirsty, eating when hungry, and fleeing when threatened.\nGPT-4o-mini finishes last. At 33.7 average steps, the LLM agent is outperformed by every other agent‚Äîincluding the random baseline. Every single LLM death was from starvation, suggesting the model understands it should drink water but consistently fails to forage for food. It reasons eloquently about trade-offs but cannot translate that reasoning into consistent action.\nSwarm agents explore more but die faster. GWO and PSO visit 23+ unique cells with exploration rates above 64%, compared to the heuristic‚Äôs 17 cells at 24%. The swarm agents are better explorers but worse survivors. They optimize for the trophy (exploration) at the expense of staying alive.\nACO is the best swarm agent. At 60.6 steps, ACO outperforms the other three swarm algorithms by 50%. Its pheromone memory creates persistent trails that implicitly encode safe paths, giving it an advantage in repeated traversal.\nRandom is not the worst. The random agent (51.8 steps) outlives PSO, GWO, WOA, and the LLM. Optimization without survival awareness is worse than no optimization at all. And apparently, so is reasoning without survival instinct.\nNobody finds the trophy. Across all 70 evaluation runs (7 agents x 10 seeds), zero trophies were found. The hunt scenario is genuinely hard‚Äî6 hunters with variable detection radii on a foggy 80x60 map with 1.3x resource drain."
  },
  {
    "objectID": "posts/2026-02-22-can-llms-survive-in-the-wild.html#performance-across-the-biomes",
    "href": "posts/2026-02-22-can-llms-survive-in-the-wild.html#performance-across-the-biomes",
    "title": "Can LLMs Survive in the Wild?",
    "section": "Performance across the Biomes",
    "text": "Performance across the Biomes\nThe hunt scenario is the hardest, but how do these agents perform across different environments? Each biome presents a unique survival challenge: forests are resource-rich but crawling with wildlife, deserts drain hydration mercilessly, and the tundra freezes everything.\n\nSurvival by Environment (Avg Steps, 10 seeds)\n\nAverage steps survived across all four environments, 10 seeds per cell. Bold = best in column.\n\n\nAgent\nHunt\nForest\nDesert\nTundra\n\n\n\n\nHeuristic\n87.5\n85.2\n50.3\n39.1\n\n\nACO\n60.6\n66.7\n31.1\n14.9\n\n\nPSO\n42.0\n52.5\n28.5\n15.5\n\n\nGPT-4o-mini\n33.7\n41.7\n31.7\n16.0\n\n\n\n\n\nVisual Breakdown by Environment\nThe table below shows each agent navigating each biome. Watch how the same algorithm behaves differently depending on the terrain, climate, and threats it faces.\n\nHeuristicGPT-4o-miniPSOACO\n\n\n\n\n\n\n\nForest (85.2 steps)\n\n\n\n\n\n\nDesert (50.3 steps)\n\n\n\n\n\n\nTundra (39.1 steps)\n\n\n\n\nThe heuristic agent thrives in the forest thanks to abundant vegetation and water. In the desert, dehydration (50%) is the primary killer. The tundra is its worst nightmare, hypothermia claims 90% of its runs despite shelter-building logic. It is supposed to be the best, since it has custom hand-crafted rules for the environments.\n\n\n\n\n\n\n\nForest (41.7 steps)\n\n\n\n\n\n\nDesert (31.7 steps)\n\n\n\n\n\n\nTundra (16.0 steps)\n\n\n\n\nThe LLM struggles everywhere but shows some adaptability. In the forest, death causes are diverse (hypothermia, starvation, hunted), suggesting the model tries different strategies rather than failing the same way every time. In the tundra, 90% of deaths are hypothermia, it cannot figure out shelter-building.\n\n\n\n\n\n\n\nForest (52.5 steps)\n\n\n\n\n\n\nDesert (28.5 steps)\n\n\n\n\n\n\nTundra (15.5 steps)\n\n\n\n\nPSO explores aggressively in every biome - note how particles spread across the map. In the forest this works reasonably well (52.5 steps). In the tundra, the entire population freezes to death within 15 steps. The particles cannot evaluate temperature risk.\n\n\n\n\n\n\n\nForest (66.7 steps)\n\n\n\n\n\n\nDesert (31.1 steps)\n\n\n\n\n\n\nTundra (14.9 steps)\n\n\n\n\nACO is the best swarm agent in both the hunt and forest scenarios thanks to pheromone memory. But in extreme environments (desert, tundra), pheromone trails are useless‚Äîthe organism dies before ants can establish meaningful paths.\n\n\n\n\n\nWhat the Biomes Reveal\nForest is forgiving. All agents survive longest here. Abundant vegetation, water, and moderate temperatures provide a buffer against poor decisions. The heuristic and ACO both break 65+ steps.\nDesert punishes poor hydration management. Dehydration is the leading cause of death across all agents. The heuristic survives twice as long because its priority-1 rule is: ‚Äúif thirsty, drink.‚Äù The LLM often knows it should drink but moves toward the trophy instead.\nTundra is the great equalizer. Every agent converges to roughly 14-16 steps, with the sole exception of the heuristic at 39. The tundra tests one thing: can you build a shelter before you freeze? The heuristic has an explicit rule for this. Nobody else does."
  },
  {
    "objectID": "posts/2026-02-22-can-llms-survive-in-the-wild.html#conclusion-and-future-steps",
    "href": "posts/2026-02-22-can-llms-survive-in-the-wild.html#conclusion-and-future-steps",
    "title": "Can LLMs Survive in the Wild?",
    "section": "Conclusion and Future Steps",
    "text": "Conclusion and Future Steps\nThe results are brutally clear: GPT-4o-mini cannot survive in the wild. Not even as well as random chance. At least not with the current set of prompts and without specific reasoning-harness.\nThe heuristic agent lives the longest because it never takes risks. It drinks, eats, rests, and runs, but it never finds the trophy. The swarm agents explore aggressively but burn through resources. The LLM agent can reason about trade-offs in a way that neither heuristics nor swarm algorithms can- its action justifications are often insightful, but translating that reasoning into consistently good actions remains a wide-open challenge. Knowing what to do and doing it are, it turns out, very different things.\n\nWhat Earth-33 Can Test Going Forward\n\nLong-horizon planning: Future LLM agents could be given memory buffers and multi-turn planning windows. Can an LLM that remembers the last 50 turns outperform one that only sees the current state?\nTool use under pressure: What if the LLM agent could ‚Äúcall‚Äù sub-routines, use a pathfinding algorithm for navigation while reserving its reasoning for high-level strategy? This mirrors how humans delegate routine tasks to muscle memory.\nMulti-agent cooperation: Earth-33‚Äôs architecture supports multiple organisms. Can two LLM agents coordinate survival better than two heuristic agents? Can they develop communication protocols through the SIGNAL action?\nTransfer across biomes: An agent trained on forest scenarios must adapt to desert or tundra. Does an LLM generalize better than a heuristic that was tuned for one environment?\nEvolutionary fine-tuning: The swarm algorithms hint at a possibility‚Äîwhat if we used evolutionary strategies to fine-tune LLM prompts or LoRA adapters, selecting for agents that survive longest?\nReal-world grounding: Earth-33‚Äôs terrain generation can be extended to use real GIS data. Imagine dropping an LLM agent into a simulation of the Sahara or the Amazon and asking it to survive. The gap between textbook knowledge and embodied intelligence would become starkly visible.\n\nThey way I intended Earth-33 doesn‚Äôt ask ‚Äúhow smart is this model?‚Äù but ‚Äúhow well does intelligence translate into action?‚Äù And right now, the answer is: not as well as we would like. But that is precisely what makes it a useful benchmark in my opinion.\nThe wilderness does not grade on a curve sadly :)\n\nEarth-33 is open source. You can find the full codebase, configs, and evaluation scripts on GitHub."
  },
  {
    "objectID": "posts/2023-12-07-moe-for-dummies.html",
    "href": "posts/2023-12-07-moe-for-dummies.html",
    "title": "Mixture of Experts for Dummies",
    "section": "",
    "text": "Mixture of Experts for Dummies\nThis post is meant as a tutorial to help one get started with the basic concept of a Mixture-of-Expert. It looks at various types of MoEs and their individual nuances. Read on to get into the mix!\n\nIn the beginning‚Ä¶\nLarge Language Models (LLMs) are powerful neural networks that have achieved remarkable results in various natural language processing tasks. However, their massive size and computational complexity pose challenges for training and deployment. To address some of these challenges, Mixture of Experts (MoE) architecture has emerged as a promising technique.\n\n\nMoE, MoE?\nMoE or Mixture of Experts is a neural network architecture that divides a large model into smaller, specialized sub-models called experts. Each expert is trained to handle specific subtasks or input types. During inference, a gating network decides which expert(s) are best suited for each input, and only those experts are activated. This allows for more efficient computation and resource allocation compared to a single large model.\n\n\nHow does a MoE work?\nMoE consists of three main components: - Expert networks: These are smaller sub-models trained on specific subtasks or input types. - Gating network: This network determines which expert(s) are best suited for each input. It considers features of the input and the capabilities of each expert. - Combiner: This component aggregates the outputs from the activated experts to produce the final output of the model.\nHere‚Äôs a simplified breakdown of the MoE process:\n\nInput: The LLM receives an input (e.g., text, code, etc.).\nFeature extraction: Features are extracted from the input.\nGating network: The gating network analyzes the features and activates a small subset of experts.\nExpert processing: Each activated expert processes the input and generates its own output.\nCombining: The outputs from the activated experts are combined to produce the final output of the LLM.\n\n\\[ \\text{Output} = \\sum_{i=1}^N \\text{Expert}_i(\\text{Input}) \\cdot \\text{Gating}(i) \\]\n\n\n\n\nVanilla MoE\nA Vanilla MoE is the simplest form of the architecture; it is a simple MoE with all the Experts switched on. We can get a brief idea from the image shown here:\n\n\n\nvanilla-moe\n\n\n\n\nSparse MoE:\nImagine a team of experts working on a complex problem. Each expert has unique knowledge and skills, but it‚Äôs not efficient to involve all of them for every task.\nSparse MoE (Mixture of Experts): Google Brain proposed a solution: a network with many ‚Äúexperts,‚Äù but only a few are active for each task. This allows for a larger model capacity while saving resources.\n\n\n\nsparse-moe\n\n\nGoal: Achieve ‚Äúsingle sample single expert processing.‚Äù This means the model chooses one specific expert for each input during inference, saving computation. Example: Imagine a team of 100 experts. The model only activates 3 experts for a specific task, significantly reducing computational cost.\nDuring training, the model tends to favor ‚Äúearlier‚Äù experts, making them more likely to be chosen. This leads to only a few experts being used effectively. This is called the ‚ÄúExpert Balancing problem.‚Äù\n\n\nTransformer MoE\nWhen models reached hundreds of billions of parameters, scaling became difficult. MoE (Mixture of Experts) resurfaced as an economical and practical solution. Google‚Äôs GShard pioneered MoE integration with Transformers. Subsequent work like Switch Transformer and GLaM further improved the technique. MoE reduced LLM parameters from billions to trillions. GLaM‚Äôs architecture:\n\n\n\ntransformer-moe\n\n\n\nMoE layers (position-wise) interweave with FFN layers in the Transformer encoder and decoder.\nTop-2 routing in the Gating Network selects the two most likely experts.\n\n\n\nLifelong-MoE\nTo tackle Lifelong Learning Google released Lifelong-MoE in May 2023. The model‚Äôs Lifelong learning strategy includes the following steps: - Expand the number of Experts and the corresponding Gating dimensions. - Freeze the old Expert and the corresponding Gating dimension, and only train the new Expert. - Use the Output Regularization method to ensure that the new Expert inherits the knowledge learned in the past\n\n\nQuiz Time!\n\n\n\n\nWrapping Up‚Ä¶\nThink of MoE as a group of scientists working on a complex project. Each scientist has their own expertise and focuses on a specific task. There is also a lead scientist, who chooses which project to push and which to halt based on their features.\nThis allows the entire project to be completed more efficiently and effectively.\nMoE is a relatively new technique in the field of LLMs, so the research is voluminious and growing each day. However, its potential benefits are significant, and it is expected to play a major role in the future development of LLMs.\n\n\nReferences and Acknowledgement\n\nThis work was developed along with Aritra Roy Gosthipaty\nThe Next LLMs Development: Mixture-of-Experts with ‚Ä¶ - AIFT\nMixture of Experts-Introduction - Abdulkader Helwan\nMixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models\nMemory Augmented Language Models through Mixture of Word Experts\n\n{% if page.comments %}\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus.\n\n{% endif %}"
  },
  {
    "objectID": "posts/2021-09-14-why-model-decay-using-the-exponential-function.html",
    "href": "posts/2021-09-14-why-model-decay-using-the-exponential-function.html",
    "title": "Why model decay using the exponential function?",
    "section": "",
    "text": "So you have been going through mathematical models and equations for all sorts of things and everywhere you go you see this equation.\nObviously, you know why it‚Äôs used. It is the infamous exponential decay function. But you want to know why specifically this function. What is so special about an exponent that it is used to model decay.\n\\[e^{-x}\\]\nIn this blog we will learn the following:\n\nWhat is a decay function?\nWhy use exponentials?\nHow does it help us model systems?\n\nLet‚Äôs get started with some definitions first.\n\nWhat is a decay function?\nWell in simple english it means to grow old and wither away from existence.\nIn scientific terms, decay is the process of reducing an amount by a certain measure consistently over a period of time.Let us look at some examples.\n\\[f(x) = (5-x)\\]\n\n\n\ndecreasing\n\n\nIn this function, we can see that for all positive values of \\[x\\] the function decreases from 5. However, if the interval is changed and x starts from 0 and goes till \\[-\\infty\\] then the function starts increasing from 5.\nWe can play around with lots of definitions and functions to see where a function would be decreasing and where it would be strictly decreasing.\n\nA function would be called strictly decreasing in the rang \\[(a,b)\\] if\n\n\\[x_1,x_2 \\in (a,b) : x_1 &lt;x_2 \\implies f(x_1) &gt; f(x_2)\\]\n\n\nWhy use exponentials?\nFirst, let us understand what kind of data is best modeled by exponentials. The following is a table of some data points.\n\n\n\nx\ny\n\n\n\n\n10\n10\n\n\n20\n20\n\n\n30\n40\n\n\n40\n80\n\n\n\nNow as we plot this data we can see that it doubles with every iteration, compounding at a constant rate. This rate of compounding is described by \\[e^x\\].\nSo we can go as far as to say that exponential is the natural language of growth. Now, what happens when we use this same idea but reverse it.\nAgain, let us look at a table of data.\n\n\n\nx\ny\n\n\n\n\n10\n10\n\n\n20\n5\n\n\n30\n2.5\n\n\n40\n1.25\n\n\n\nThe graph for this data would be something like this:\n\n\n\ndecreasing\n\n\nThis can be modeled by the equation \\[y= e^{-x}\\]\nBut why? Why does this equation fit this data so well? To understand this better let us go back to our old friend - rate-of-change.\nNow how do we describe decay through calculus? Well, we need to take a quantity \\[N\\] and another quantity \\[x\\]. And we need to write how the rate of change of \\[N\\] varies with the rate of change of \\[x\\].\n\\[\\frac{dN}{dx}\\]\nWe can say that this rate of change may be something as simple as \\[-bN\\] where \\[b\\] is a constant.\n\\[\\frac{dN}{dx} = -bN \\\\\n\\]\nThis equation can be rewritten and solved fairly simply\n\\[\\frac{dN}{N} = -bx \\\\\nln(N) = -bx+C\\]\nSolving for the logarithm and ignoring the constant of integration we have\n\\[N(x) = N_oe^{-bx}\\]\nWhere \\[N(x)\\] is the current value of \\[N\\] with respect to \\[x\\] and \\[N_0\\] is the initial value or the starting point of \\[N\\]\n\n\nHow does it help us model systems?\nNow we know that the function \\[A(x) = A_0e^{-bx}\\] can very easily model the process of compounding decay.\nThe use of this equation in modeling real-world applications is numerous.\n\nIt is particularly useful in modelling a certain kind of decay\nIt is easy to determine when the decaying quantity would be half of its original amount. This is commonly known as half-tim.\nThe function is also easily differentiable with it‚Äôs rate of change being the same as the function itself.\n\nThe function \\[y= e^{-x}\\] is a powerful tool to have in your arsenal. But it is even more powerful when we understand when and where to use it best. Not all systems that decay gradually can be modelled using the exponential decay function, the trick is to use it selectively and use it best.\n\n\nReferences\n\nhttps://www.thoughtco.com/exponential-decay-definition-2312215#:~:text=In mathematics%2C exponential decay describes,of time that has passed.\nhttps://courses.lumenlearning.com/waymakercollegealgebra/chapter/exponential-growth-and-decay/\nhttps://mathworld.wolfram.com/DecreasingFunction.html"
  },
  {
    "objectID": "posts/2026-02-21-mathematical-mysteries-of-the-starry-night.html",
    "href": "posts/2026-02-21-mathematical-mysteries-of-the-starry-night.html",
    "title": "Mathematical Mysteries of the Starry Night",
    "section": "",
    "text": "An answer to a curiousity long fostered in a brain that can attempt to appreciate if not understand art."
  },
  {
    "objectID": "posts/2026-02-21-mathematical-mysteries-of-the-starry-night.html#modelling-gravity-in-a-world-that-van-gogh-saw.",
    "href": "posts/2026-02-21-mathematical-mysteries-of-the-starry-night.html#modelling-gravity-in-a-world-that-van-gogh-saw.",
    "title": "Mathematical Mysteries of the Starry Night",
    "section": "Modelling Gravity in a world that Van Gogh saw.",
    "text": "Modelling Gravity in a world that Van Gogh saw.\nWhen Vincent van Gogh painted The Starry Night in 1889, looking out from the window of his asylum, I believe he experienced some form of epiphany whereby some incredible secret of the unknown cosmos presented itself to him.\n\n\n\nA vision that can only happen once or twice in human history.\n\n\nFor decades, art historians viewed these iconic swirls as mere representations of Van Gogh‚Äôs turbulent mental state. But recently, physicists noticed something startling. The patterns in the brushstrokes‚Äîspecifically the way the brightness fluctuates across the canvas‚Äîclosely mimic the mathematical structures of real-world fluid turbulence.\nIn my own way, I have always been fascinated with this and even did some preliminary explorations in my undergrad days.\n\n\n\nSmall Efforts into the Night\n\n\nNeedless to say, because of my own shortcomings and the absence of vibe-coding I could only explore so much as creating cyclic graphs from the illumination pattern.\nBut what if the universe actually worked the way Van Gogh painted it? What if gravity wasn‚Äôt a smooth pull, but a turbulent, swirling ocean?\nTo understand the chaos, we must first understand the order.\n\n\nStandard Gravity\nOur current understanding of solar systems relies on stability. Isaac Newton described gravity as a universal force of attraction that is predictable and smooth.\nIn a standard planetary system, a planet is constantly falling toward its star. However, it also has tremendous forward momentum. The resulting orbit is a perfect balance between the star trying to pull the planet in, and the planet trying to fly off in a straight line.\nIn the interactive simulation below, we see a simple, orderly system. The blue arrow represents the gravitational force vector‚Äîthe direction and intensity with which the star is pulling the planet. Notice how the arrow always points directly to the center, and gets longer (stronger pull) when the planet is closer.\n\nA simple gravitational system.\n\n\n  \n  Standard Gravity: The force vector (blue arrow) always points smoothly toward the star.\n\n\n\n\n\n\n\nKeppler‚Äôs Laws of Motion\nBecause gravity is smooth and central, planetary motion follows incredibly precise rules discovered by Johannes Kepler in the early 17th century. These laws are the bedrock of orbital mechanics.\n\nThe Law of Ellipses: All planets move in elliptical orbits, with the sun at one focus.\nThe Law of Equal Areas: A line that connects a planet to the sun sweeps out equal areas in equal times. (This means planets speed up when closer to the star and slow down when further away).\nThe Law of Harmonies: The square of the orbital period of a planet is directly proportional to the cube of the semi-major axis of its orbit.\n\nExpressed mathematically, the third law looks like this:\n\\[T^2 = \\frac{4\\pi^2}{GM} a^3\\]\nWhere \\(T\\) is the orbital period, \\(a\\) is the average distance (semi-major axis), \\(G\\) is the gravitational constant, and \\(M\\) is the mass of the star.\nBecause of these laws, we can have complex, stable systems with multiple planets. They don‚Äôt collide because the smooth gravitational field keeps them in their predictable ‚Äúlanes.‚Äù\n\nA multi-planet gravitational system\n\n\n  \n  A stable Keplerian system. Multiple planets maintain distinct orbits because the central force is uniform.\n\n\n\n\n\n\n\nModelling turbulent gravity\nNow, back to Van Gogh. The patterns in The Starry Night align with a theory proposed by mathematician Andrey Kolmogorov in 1941 regarding fluid turbulence.\nKolmogorov described an ‚Äúenergy cascade.‚Äù Imagine stirring a cup of coffee. You create large swirls (eddies). Those large swirls become unstable and break into smaller swirls, which break into even smaller ones, until the energy is lost to heat.\nKolmogorov found a specific mathematical relationship defining how much energy exists at different sizes of these swirls. The formula for the energy spectrum \\(E(k)\\) is:\n\\[E(k) = C \\epsilon^{2/3} k^{-5/3}\\]\nHere, \\(k\\) relates to the size of the swirl (wavenumber), and the key feature is the \\(-5/3\\) exponent. Amazingly, Van Gogh‚Äôs brushstrokes scale according to this exact ratio.\n\n\n\n  \n\n\n\n\nA Universe of Turbulent Gravity\nIf we take Van Gogh‚Äôs vision literally and apply Kolmogorov‚Äôs turbulence to gravity, the universe changes drastically.\nSpacetime would no longer be a smooth slope curving toward a star. It would be a churning ocean of gravitational ‚Äúeddies.‚Äù\nIn such a system:\n\nThe force vector would not always point at the star. As a planet moves through gravitational eddies, it would experience sideways shoves and sudden drops in pull.\nThere are no stable lanes. The smooth mathematical certainty of Kepler‚Äôs laws evaporates.\n\nTo be perfectly accurate, there is no formal physical theory called ‚ÄúKolmogorov Gravity.‚Äù What we are doing in this thought experiment is a mathematical synthesis: we are taking the established laws of Newtonian Gravity and perturbing them using a vector field defined by the Kolmogorov 1941 (K41) theory of fluid turbulence.\nIt is a beautiful way to visualize how a chaotic energy cascade would warp the fabric of spacetime.\n\n\n\n\nThe Mathematics of ‚ÄúTurbulent Gravity‚Äù\nThe foundation of Kolmogorov‚Äôs theory is the energy spectrum equation, which describes how kinetic energy is distributed across eddies of different sizes (wavenumbers, \\(k\\)):\n\\[E(k) = C \\epsilon^{2/3} k^{-5/3}\\]\nTo apply this to a gravitational field, we need to convert that energy \\(E(k)\\) into a force or acceleration perturbation amplitude, \\(A(k)\\). Because kinetic energy is proportional to velocity squared (\\(E \\propto v^2\\)), the amplitude of the vector perturbation at a specific wavenumber scales to the square root of the energy spectrum:\n\\[A(k) \\propto \\sqrt{k^{-5/3}} = k^{-5/6}\\]\nThis \\(k^{-5/6}\\) relationship is the mathematical heart of the widget.\nFinally, we define the total gravitational vector field, \\(\\vec{g}_{total}\\), experienced by the planet at position \\(\\vec{r}\\) and time \\(t\\). It is the sum of the smooth Newtonian central force and the turbulent spatial perturbation \\(\\vec{T}\\):\n\\[\\vec{g}_{total} = -\\frac{GM}{|\\vec{r}|^2} \\hat{r} + \\vec{T}(\\vec{r}, t, k)\\]\nTo mimic the swirling nature of fluid dynamics in 2D space without running a heavy Navier-Stokes physics engine, \\(\\vec{T}\\) is modeled using out-of-phase trigonometric harmonics, scaled by the wavenumber \\(k\\) and the amplitude \\(A(k)\\):\n\\[\\vec{T}(\\vec{r}, t, k) = A(k) \\begin{bmatrix} \\sin(k r_x + \\omega t) \\cos(1.5 k r_y + \\omega t) \\\\ \\cos(k r_y + \\omega t) \\sin(1.5 k r_x - \\omega t) \\end{bmatrix}\\]\n\n\n\nThe Visual Result\nBecause of the \\(k^{-5/6}\\) amplitude rule, the slider behaves counter-intuitively‚Äîbut mathematically perfectly:\n\nLow Wavenumber (e.g., 0.5): The eddies are spatially massive, but because \\(k\\) is small, the amplitude is very high. The planet gets caught in massive, powerful spatial tides that drastically warp its orbit.\nHigh Wavenumber (e.g., 4.0): The space fractures into high-frequency, tiny micro-swirls. However, because \\(k\\) is large, the amplitude drops significantly. The planet ‚Äújitters‚Äù through the field but maintains its overall circular trajectory much longer because the chaotic forces are much weaker.\n\n\n\n\n  \n    \n    \n    \n      \n        \n          Wavenumber (k) - Eddy Spatial Frequency\n          1.0\n        \n        \n      \n      Reset Orbit\n    \n  \n\n\n\nIn the final simulation below, we model this ‚ÄúVan Gogh Gravity.‚Äù We take the standard central pull of the star and add ‚Äúturbulent noise‚Äù to it based on the planet‚Äôs position. The force vectors (the faint white arrows) no longer point straight to the center; they wiggle and twist chaotically.\nWatch what happens to the planetary orbits.\n\nA gravitational system that swirls\n\n\n  \n  Turbulent Gravity: Gravitational \"eddies\" perturb the force vectors. Keplerian orbits are impossible; planets are eventually ejected or consumed.\n  Reset Simulation\n\n\n\nThe result is beautiful chaos. The planets cannot maintain stable ellipses. They are buffeted by gravitational winds, their orbits becoming unpredictable squiggles until they are either flung into deep space or crash into the star.\nVan Gogh was not a physicist. And thanks the god for that. He was able to see and capture what few men have seen and experienced in human history. It is a coincidence that his art can be expressed as an exact form of turbulence.\nHowever turbulence is not a favorable phenemenon when creating forces of nature that render our universe heterogeneous.\nIt is however for the more curios minds amongst us to wonder how our universe‚Äôs simulation is coded. And how far we are from recreating it?"
  },
  {
    "objectID": "posts/2021-04-12-what-do-we-mean-when-we-talk-about-causal-inference.html",
    "href": "posts/2021-04-12-what-do-we-mean-when-we-talk-about-causal-inference.html",
    "title": "What do we mean when we talk about Causal Inference?",
    "section": "",
    "text": "So what does Causal Inference really mean? What does it mean to cause? Is Causality really as simple as understanding cause and effect? Let us attempt to first understand few of the ground rules before we play the game of Causality. \nIf you had a penny for every time you heard that line, you would probably be Bruce Wayne by now. But what does it mean? Why does correlation of two events not imply causality? What is up?\n\nAt this point you‚Äôre probably wondering, yeah I have heard about Causality a few hundred times, but do I really need to care?\n\nLet me ask you a different question then.\nHow many times have you looked at the result of your model and wondered what-if the data was something other than what I trained on. Maybe you write an algorithm that predicts the sales of comic books, and your model works really well and produces high accuracy predictions, but you need to know why. Or maybe its the opposite, your algorithm predicts completely wrong sales figures and you really need to figure out a reason for that.\nConfused? Let‚Äôs look at an example-\nSuppose you are hired by Marvel as a data scientist. There has been a recent rise in comic book sales and you need to figure out the reason so that the company can mantain the sales figures. After some data analysis you come to the conclusion that there is a direct correlation between comic book sales and disney plus subscriptions. But you still don‚Äôt have an exact reason, so you come up with some scenarios to form a hypotheses.\n\nScenario 1\n The comics display ads from disney, so obviously an increase in comic book sales causes more Disney Plus subscriptions.\n\n\nScenario 2\n Disney Plus has shows about comic book characters. New fans would like to consume more of such content. So increase in disney plus subscriptions lead to more marvel comics being bought.\n\n\nScenario 3\n Or maybe its something quite different. All Marvel Cinematic Universe movies are on Disney Plus. When a new Marvel movie comes out the hype around these characters lead to more comic book sales (and in turn more Disney Plus subscriptions).\n\nThe possibility presented in scenario 3 is what is termed as a hidden cause.\n\nEach node is a variable and each arrow shows the direction of causal connection.\n\nSuppose we build an accurate model to predict when the user will buy more comic books. But that is all that our model would do. When in reality, the actual problem statement could be better expressed by a question like-\n\nWhat would the user have done if we had done something differently?\n\nSo you might be thinking, I get it the model doesn‚Äôt always take hidden causes into account, but it still works. The predictions are still accurate.\nWell, it is actually considered really lucky to have our observed effect and causal effect in the same direction. More often than not we see the opposite in real world data.\n\nLets take a look at another example to understand this better-\nSay suppose you have now taken up employment at a food ordering app as a Data Scientist. The product designer wants to introduce a new User Interface which she believes will engage more users. She has introduced the new UI to a select few people and wants you to make sense of the data that has been gathered. You decide to judge the the Old User Interface and the new one on a common criteria, which is whether the user orders after opening the app. Lets call this the opened-and-ordered criteria.\n\n\n\n\n\n\n\n\nUI Type\nRatio of users who opened-and-ordered to total users\nPercentage\n\n\n\n\nOld UI\n20/100\n20%\n\n\nNew UI\n28/100\n28%\n\n\n\nBased on this data, it seems pretty straightforward. The new UI is the clear winner. But something very strange happens when you condition the data on high activity vs low activity users.\n\n\n\n\n\n\n\n\n\nOld UI\nNew UI\n\n\n\n\nPercentage of Opened-and-Ordered for High Activity Users\n15/80 = 25%\n14/60 = 23.33%\n\n\nPercentage of Opened-and-Ordered for Low Activity Users\n2/20 = 10%\n5/40 = 7.5%\n\n\n\nThis discrepancy of data on being modeled on different subset of the original data is called ‚ÄúSimpson‚Äôs Paradox‚Äù. Google this later. :wink:\n\n‚ÄúBut hey why does this discrepancy occur?‚Äù\n\nWell maybe, since the old UI is already tried and tested it is shown to users only at a particular time of the day, around evening when app activity is generally high. The new UI might be shown at other times of the day when the number of lower activity users who are likely to order food is actually less.\nSo the Causal diagram may look something like this:\n\nThe debate on what it means to cause something and how cause precedes effect has been going on forever. Most prominent contestants include but are not limited to -\n\nAristotle\nHume\nEinstein\nThat French guy from Matrix 3\nJudea Pearl and many more\n\nHowever sadly until recently, the science of causal inference was not considered formal mathematics or even something that could be quantified and studied.\nThis was remedied hugely by the contributions of Judea Pearl (Causal Graphical Models) and Donald Rubin (Rubin Causal Model).\nSo in conclusion, what we really learned here was that simply because two variables are correlated does not necessarily mean that one is the direct cause of the other. If we do not keep an eye out for hidden causes and their effects we can actually do more harm through our model.\nTherefore the idea of what would happen had something in the observation process been changed becomes an imperative question to ask.\nThis is called Counterfactual Thinking.\n\nCounterfactual thinking, or specifically thinking about what-if scenarios is the very heart of causal inference.\n\nSome of the ways we can answer a counterfactual question are -\n\nRandomization\nNatural Experiments\nConditioning\n\nInetrestingly as we go down the list, the methods become much harder to use. But as we go up, the methods are more foolproof and give better validation results. We will take a closer look at these and the various underlying methodologies in the next blogpost, till then keep learning and don‚Äôt stop asking the question -\n\nWhat-If?\n\n\n\n\nReferences\n\nCausal Inference in Online Systems by Amit Sharma\nAn awesome playlist for Causal Learning"
  },
  {
    "objectID": "posts/2021-08-10-a-brief-introduction-to-do-calculus.html",
    "href": "posts/2021-08-10-a-brief-introduction-to-do-calculus.html",
    "title": "A Brief Introduction to Do-Calculus",
    "section": "",
    "text": "Welcome back to Part 3 of The Causal Blog. The previous two parts introduced us to the world of causal inference and what are the various methodologies involved. You can find them right here.\nIn this part we will be discussing a very popular and useful method known as the do-calculus developed by Judea Pearl in 1995. It was developed to propose a foolproof methodology for identification of causal effects in non parametric models.\nWell, that‚Äôs a mouthful. What do we mean by that?\nIn simple words, this means to identify the effect or effects for a particular cause from data that is continuous rather than having discrete values.\nWe have learned in the previous blogs that it is impossible to do Causal Inference without having some form of intervention on the provided data. To facilitate this do-calculus introduces a mathematical operator called \\(do(x)\\) which simulates intervention by removing certain functions from the model and by replacing them with a constant \\(X=x\\). To understand how this plays out we will first have to look at some of the definitions introduced by the authors.\n\nDefinitions and Rules\n\n1. Definition 1\nThe probability distribution of the outcome \\[Y\\] after the intervention is given by the equation:\n\\[\nP_M(y \\mid do(x))=P_{M_x}(y)\n\\]\nwhere the distribution of the outcome \\[Y\\] is defined as the probability assigned by the model \\[M_x\\] to each outcome level \\[Y=y\\]\n\n\n2. Definition 2\nThis part talks about when and under what conditions a causal query( whether a variable or a group of variables is the cause for a given effect or not) is identifiable.\nGiven a set of assumptions \\(A\\) satisfy two fully specified models \\[M_1\\] and \\[M_2\\], the following is the criteria for identifiability:\n\\[P(M_1) = P(M_2) =&gt; Q(M_1) = Q(M_2)\\]\nThis means that whatever the details of the models are, if the distribution of the two models given the same set of assumptions \\(A\\) are equal then it follows that the causal query for the two models should also be equal. This can be extended to mean that a causal query, under such circumstances can be expressed in terms of the parameters of \\(P\\).\n\n\n\nThe 3 Rules of do-calculus\nNow that we have learned about the definitions of do-calculus let us familiarise ourselves with the three rules that govern the mathematics of do-calculus. But first we need to understand the necessity of these rules.\nIn the previous section we learned under what conditions a causal query will be identifiable and we also saw how to formulate an expression in terms of a do-expression, e.g \\[P_M(y \\mid do(x))=P_{M_x}(y)\\]. So when a causal query is given to us in the form of do-expression there are actual mathematical steps that can be taken to resolve it and find out whether the query is identifiable or not.\nConsider the following directed acyclic graph \\[G\\] where \\[X\\],\\[Y\\],\\[Z\\] and \\[W\\] are arbitrary disjoint nodes. \\[G_{\\bar{X}}\\] is the manipulated graph where all incoming edges to \\[X\\] have been removed.\n\n\n\nexample1\n\n\nSimilarly \\[G_{\\underline{X}}\\] is the manipulated graph where all outgoing edges to \\[X\\] have been removed.\n\n\n\nexample2\n\n\nAnother useful notation to get familiarised with is the concept of d-separation (\\[\\perp\\perp\\]). In very simple words, given the graph \\[a \\rightarrow c \\rightarrow b\\] the expression \\[ a \\perp\\perp b \\mid c\\] means that a is conditionally independent of b given c. To understand d-separation in a more detailed manner have a look at this single page explanation.\n\nRule 1: Insertion/deletion of observation\n\\[P(y \\mid do(x),z,w)\\] = \\[P(y \\mid do(x),w)\\] if \\[(Y \\perp\\perp Z \\mid X,W)\\] for \\[G_{\\bar{X}}\\]\nThis means that if \\[Y\\] is d-separated from \\[Z\\] given \\[X\\] and \\[W\\] then the expression of probability \\[P(y \\mid do(x),z,w)\\] resolves to \\[P(y \\mid do(x),w)\\]. An easier way to understand this is by getting rid of the do-operators on both the sides of the equality sign.\n\\[P(y \\mid z,w)\\] = \\[P(y \\mid w)\\] if \\[(Y \\perp\\perp Z \\mid W)\\] for \\[G\\]\nThe above expression simply implies conditional independence within the variables in the distribution given regular d-separation.\n\n\nRule 2: Action/observation exchange\n\\[P(y \\mid do(x),do(z),w)\\] = \\[P(y \\mid do(x),z,w)\\] if \\[(Y \\perp\\perp Z \\mid X,W)\\] for \\[G_{\\bar{X}\\underline{Z}}\\]\nTo simplify the expression above let us again remove \\(do(x)\\) or consider \\(X\\) to be an empty set.\n\\[P(y \\mid do(z),w)\\] = \\[P(y \\mid z,w)\\] if \\[(Y \\perp\\perp Z \\mid W)\\] for \\[G_{\\underline{Z}}\\]\nThis expression refers to the backdoor-adjustment criteria that we saw in chapter 2. Therefore this rule gives us the interventional distribution for the backdoor adjustment criteria.\n\n\nRule 3: Insertion/deletion of action\n\\[P(y \\mid do(x),do(z),w)\\] = \\[P(y \\mid do(x),w)\\] if \\[(Y \\perp\\perp Z \\mid X,W)\\] for \\[G_{\\bar{X}\\bar{Z(W)}}\\]\nwhere \\[Z(W)\\] is the set of \\[Z\\] nodes that are not ancestors of any \\[W\\] node in \\[G_{\\bar{X}}\\].\nAgain for the sake of simplification let us remove the \\(do(x)\\) operator from the above expression.\n\\[P(y \\mid do(z),w)\\] = \\[P(y \\mid w)\\] if \\[(Y \\perp\\perp Z \\mid W)\\] for \\[G_{\\bar{Z(W)}}\\]\nNow let us take a moment to pause here and really understand what this means. On the paper it means that we do can remove the intervention term \\[do(z)\\] provided there is no causal association flowing from Z to Y \\[(Y \\perp\\perp Z \\mid W)\\] in the graph \\[G_{\\bar{Z(W)}}\\].\nBut that‚Äôs not all. We have a strange term called Z(W) which doesn‚Äôt quite fit in.\nThe simplified expression should have been :\n\\[P(y \\mid do(z),w)\\] = \\[P(y \\mid w)\\] if \\[(Y \\perp\\perp Z \\mid W)\\] for \\[G_{\\bar{Z}}\\]\nWhere removal of incoming edges to Z should result in d-separation of Y and Z and no causal association should flow from Z to Y. However instead of this simple term we end up with an expression containing Z(W). To understand this better let us consider the graph below:\n\n\n\nexample3\n\n\nNow the intuitive idea is to remove incoming edges to Z (\\[G_{\\bar{Z}}\\]). But if we do that then we risk changing the distribution of Y altogether through the backdoor path consisting of U and V.\nInstead what we can do is take a sub-node of Z say \\[Z_2\\] which is not an ancestor of any node in W and then remove all the incoming edges to it (\\[G_{\\bar{Z_2}}\\]). This is shown in the figure below.\n\n\n\nexample4\n\n\n\n\n\nConclusion\nThe rules and definitions of do-calculus provide a general structure for identifying Causal queries. The final query Q should be free of any do-operator, this can be achieved by repeatedly applying the three rules. It is also complete, meaning if there exists a Causal Query Q which is identifiable then it can be identified using do-calculus.\nThis chapter was aimed at introducing do-calculus very briefly and laying down the rules of the game. The idea is to not intimidate any newcomer with a whole lot of mathematical jargon but to provide an insight into an essentially simple yet powerful framework for causal inference.\n\n\nReferences\n\nThe Do-Calculus Revisited by Judea Pearl\nPearl‚Äôs Do-Calculus by Brady Neal"
  },
  {
    "objectID": "posts/2024-12-29-notes-on-technical-writing.html",
    "href": "posts/2024-12-29-notes-on-technical-writing.html",
    "title": "Notes on Technical Writing",
    "section": "",
    "text": "Notes on Technical Writer\nSince the very start of my career, I have viewed technical writing as more than just a means of communication‚Äîit‚Äôs a powerful tool for both teaching and learning.\nThe process of organizing thoughts, and presenting them clearly often benefits the writer as much, if not more, than the reader.\nIn this post, I aim to capture some of the lessons I‚Äôve learned, the tools I rely on, and the principles I follow to make technical writing a meaningful and impactful craft.\n\n\nGeneral Rules of the Trade\n\n1. Clarity Is Key\nTechnical writing should simplify, not complicate. Follow these golden rules: - Break down instructions into small, digestible steps. - Use links, screenshots, and examples to guide the reader. - Include the expected outcomes for every process.\n\n\n2. T-Shaped Approach for Deep Dives\nWhen explaining a topic: - First, provide an overview to cover the breadth of the subject. - Then, choose an aspect to dive into, offering depth and detail. - Use Google‚Äôs Style Guide for consistency.\nCore Techniques from Google‚Äôs Style Guide: - Write in the active voice to clarify actions and actors. - Use numbered lists for sequential steps and bulleted lists for non-sequential points. - Write in the second person, addressing the reader as ‚Äúyou.‚Äù - Place conditions before instructions, not after (e.g., If the file exists, then delete it). - Format code or technical terms using code font.\n\n\n\n3. Understand and Think Like Your Audience\n\nWho is your audience? Create personas with attributes such as:\n\nRole: QA Tester, Developer, or System Administrator.\nGoal: Restore a database, deploy an application.\nKnowledge Base: Familiarity with Python, command-line tools, or Linux.\n\nAdjust for inclusivity: Avoid jargon unless your audience is well-versed in it. Provide definitions for unfamiliar terms and include links to additional resources.\nStrike a balance: Avoid over-narrowing your focus to one persona. Broaden where possible to be inclusive of diverse readers.\n\n\n\n\n4. Write, Review, Improve\n\nRead it aloud: This ensures conversational and engaging writing. If sentences feel awkward, rephrase them.\nTake breaks: Step away from your draft to gain fresh perspectives.\nChange context: Print your draft or change fonts for a new perspective.\nSeek peer feedback: Like code, writing benefits from constructive reviews. Ensure your reviewers understand your style guide.\n\n\n\n\n5. Code Integration Best Practices\n\nAlways include line numbers when providing code examples and refer to them in your explanations.\nOffer a link to the full code (e.g., GitHub or Gist) at the beginning of the document.\nUse visual tools like Slides Code Highlighter to make code stand out in presentations.\n\n\n\n\n6. Use LLMs as a Tool (only a tool)\nLeverage Large Language Models (LLMs) such as ChatGPT, Claude and others to: - Edit and refine your drafts. - Refactor sections for clarity. - Never use it to think for you!\n\n\n\n\nTools to Elevate Your Writing\nHere‚Äôs a curated list of tools (that I personally use) to boost your technical writing:\n\nSlides Code Highlighter\nHighlight code elegantly in Google Slides.\nManim\nCreate high-quality technical videos and animations.\nDraw.io\nCraft polished technical diagrams.\nGrammarly\nRefine grammar, spelling, and readability.\nHemingway App\nAssess readability and improve sentence structure.\nWrite the Docs\nJoin a global community for resources, workshops, and support.\n\n\n\n\nParting Thoughts\n\n1. Prioritize the Reader\nYour audience is the heart of your writing. Focus on their needs rather than how much information you can cram in.\n\n\n2. Simplicity Is Hard\n\nKnow your audience‚Äôs knowledge level.\n\nUse clear, concise language.\n\nStructure content logically, with easy-to-follow steps.\n\n\n\n3. Community helps, a lot\nTechnical writing is not a solitary journey. Engage with communities like Write the Docs to grow, learn, and refine your craft.\nMastering technical writing is a journey, but with practice and empathy for your audience, you can transform your documentation into a tool that empowers and enlightens its readers. Keep writing, keep improving, and most importantly‚Äîkeep caring.\n\n\n\n\nReferences\n\nGoogle Technical Writing\n\nReddit: Becoming a Technical Writer\n\nSheCodeAfrica: A Guide to Technical Writing"
  },
  {
    "objectID": "posts/2023-07-24-understanding-augmix.html",
    "href": "posts/2023-07-24-understanding-augmix.html",
    "title": "Understanding AugMix",
    "section": "",
    "text": "TL;DR\nIn machine learning, we use a set of data (known as the ‚Äútraining‚Äù data) to teach an algorithm how to solve a task. In this context, we‚Äôre talking about deep neural networks which are a kind of machine learning algorithm designed to classify images. An image classifier‚Äôs job is to look at an image and decide which category it belongs to, like identifying whether a photo is of a cat or a dog.\nWhen the algorithm is learning, it adjusts itself to perform well on the training data. It‚Äôs then tested on separate ‚Äútest‚Äù data to see how well it has learned. Ideally, the training and test data should be very similar (identically distributed) in nature. For example, if the task is to distinguish between cats and dogs, and if all the images in the training set are taken in broad daylight, we expect the test set also to have images taken in similar conditions.\nHowever, in real-world scenarios, there can be a mismatch between the training and test data. This could be due to a variety of factors like the lighting conditions, the angle of the camera, or the breed of the dogs and cats in the images. When this mismatch happens, the accuracy of the image classifier can drop significantly because it has not encountered these conditions during training.\nMost current techniques for training these algorithms struggle when the test data is different from the training data in ways they didn‚Äôt anticipate. This is where the technique called ‚ÄúAUGMIX‚Äù comes in.\nAUGMIX is a method that helps improve the robustness of the model. Robustness in this context refers to the model‚Äôs ability to maintain accuracy even when the test data differs from the training data in unexpected ways.\nHere‚Äôs how AUGMIX works: it applies a mix of augmentations (slight modifications) to the images in the training data. These might include things like adjusting the brightness or contrast, rotating the image, or zooming in slightly. By doing this, AUGMIX creates a wide range of scenarios the model might encounter. It‚Äôs like showing the model not only pictures of cats and dogs taken in the day but also at twilight, from different angles, of different breeds, etc.\nThis helps in two ways:\n\nIt makes the model more robust because it has seen a wider range of image conditions during training.\nIt helps the model to provide better uncertainty estimates. Uncertainty estimates tell us how confident the model is about its predictions. A well-calibrated model knows when it‚Äôs likely to be wrong, which is very useful when decisions based on these predictions have significant consequences.\n\n‚Ä¶ (content continues, all image references updated to /assets/images/ as needed) ‚Ä¶"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ritwik Raha",
    "section": "",
    "text": "Hi, I‚Äôm Ritwik Raha, Machine Learning Engineer at Google by job title, professional neural network whisperer by life choice. I spend my time thinking about post-training, RL, and whether LLMs can be taught to reason, or at least fake it convincingly.\nWhen I‚Äôm not wrangling models, I‚Äôm either making semi-coherent ML videos on YouTube or committing mild acts of chaos on X under the noble banner of shitposting.\nThis site is my corner of the internet, equal parts portfolio, thought-dump, and polite flex.\nOh, and I am also a very human artist, invested in ‚ÄúArtists against AI‚Äù movement. Yeah I know, paradox! Go check out some of my artwork here.\n\n\n\n\nWork and News\n\n\n\n14 Jan 2025 ‚Äî Joined Google as a MLE\n\n\n22 Apr 2023 ‚Äî ML and Vibes as a GDE in ML (Keras)\n\n\n6 Feb 2022 ‚Äî Joined PyImageSearch as a MLE\n\n\n6 Feb 2021 ‚Äî Joined TCS as a MLE\n\n\n20 Jan 2020 ‚Äî Published papers in ICCE 2020\n\n\n20 Jul 2020 ‚Äî Graduated as an Instrumentation Engineer\n\n\n\n\n\nTalks\n\n\nAI Planet - LLM Bootcamp\nML Paper Reading Club Coimbatore - ResNets\nKeras Community Day Kolkata - Keras Philosophy\nKeras Community Day Durg - Keras Philosophy\nKeras - The LEGO of Framework\nDecoding DETR - TFUG Mumbai\nA Guide to ML Workflows with JAX - TFUG Kolkata\nLearning JAX in 2023 - JAX‚Äôs Power Tools\nIntroduction to Neural Radiance Fields - NYTFUG\nA Deep Dive into Transformers - PyImageSearch\nNeural Machine Translation - PyImageSearch\nIntroduction to RNNs - PyImageSearch\nEnhancing Image Resolution with GANs - PyImageSearch\nOpenCV and Deep Learning Tutorial - PyImageSearch\nIntroduction to Deep Learning - PyImageSearch\nImage Fusion with WOA-PCNN - ICCE 2020\nCausality Analysis of Emotional States from EEG Response - ICCE 2020\nIntro to ML with DSC NSEC\n\n\n\n\nTutorials\n\n\nRecent Deep Dives\n\nCan we really scale RL?\nDDPM Explained for Dummies\nChoosing between SigLIP and CLIP for Language-Image Pretraining\nUnderstanding PaLI-Gemma in 50 Minutes or Less\n\n\n\nPaper Breakdowns\n\nDETR Breakdown Part 1\nDETR Breakdown Part 2\nDETR Breakdown Part 3\n\n\n\nJAX Guide\n\nLearning JAX in 2023: Part 3\nLearning JAX in 2023: Part 2\nLearning JAX in 2023: Part 1\n\n\n\nComputer Vision\n\nFocal Modulation: A replacement for Self-Attention - Keras Example\nA Vision Transformer without Attention - Keras Example\nNeural Style Transfer with AdaIN- Keras Example\nBreaking down Neural Radiance Fields - Part 1\nBreaking down Neural Radiance Fields - Part 2\nBreaking down Neural Radiance Fields - Part 3\n3D volumetric rendering with NeRF- Keras Example\nImage Segmentation using Whale Optimization Algorithm\nImage Compression using SVD\nFace Swapping using OpenCV\nCreating a Potrait mode with Open CV\nA brief history of Edge Detection\nDenoising images the Matlab Way\n\n\n\nNLP\n\nIntroduction to RNNs with TensorFlow and Keras\nLong Short-Term Memory Networks\nNeural Machine Translation\nNeural Machine Translation with Bahdanau‚Äôs Attention Using TensorFlow and Keras\nNeural Machine Translation with Luong‚Äôs Attention Using TensorFlow and Keras\nA Deep Dive into Transformers with TensorFlow and Keras: Part 3\nA Deep Dive into Transformers with TensorFlow and Keras: Part 2\nA Deep Dive into Transformers with TensorFlow and Keras: Part 1\n\n\n\nHitchhiker‚Äôs Guide to‚Ä¶\n\nWhat is Keras Core?\nA Hello World to Deep Learning in Matlab\nAutomatic Differentiation Part 1: Understanding the Math\nAutomatic Differentiation Part 2: Implementation Using Micrograd\n\n\n\n\n\nEducation\nSome of the formal and slightly informal education that I have recieved.\n\nGraduation - B.Tech Electronics and Instrumentation, Netaji Subhash Engg. College 2016-2020\nHigh School - Sri Aurobindo Institute of Education, 2014- 2016\nSchool - Sri Aurbindo Institte of Education, 2009-2014\n\n\n\nResearch\nThe following are the links to some of my research publications.\n\nR. Raha, A. Sengupta and A. Saha, ‚ÄúCausality Analysis of Emotional States from EEG Response,‚Äù 2020 IEEE 1st International Conference for Convergence in Engineering (ICCE), Kolkata, India, 2020, pp.¬†410-415, doi: 10.1109/ICCE50343.2020.9290546.\nR. Raha, A. Sengupta and S. Dhabal, ‚ÄúMedical Image Fusion using PCNN Optimized by Whale Optimization Algorithm,‚Äù 2020 IEEE 1st International Conference for Convergence in Engineering (ICCE), Kolkata, India, 2020, pp.¬†374-378, doi: 10.1109/ICCE50343.2020.9290504.\n\nGoogle Scholar Profile"
  }
]