{"title":"What do we mean when we talk about Causal Inference?","markdown":{"yaml":{"title":"What do we mean when we talk about Causal Inference?","date":"2021-04-12","author":"Ritwik Raha","tags":["causality"],"image":"/assets/images/1post/memecausal.png"},"headingText":"Scenario 1","containsRefs":false,"markdown":"\n\nSo what does Causal Inference really mean? What does it mean to cause? Is Causality really as simple as understanding cause and effect? Let us attempt to first understand few of the ground rules before we play the game of Causality.\n![correlation does not imply causation](/assets/images/1post/memecausal.png)\n\nIf you had a penny for every time you heard that line, you would probably be Bruce Wayne by now. But what does it mean? Why does correlation of two events **not imply** causality? What is up?\n\n>At this point you're probably wondering, yeah I have heard about Causality a few hundred times, but do I really need to care?\n\nLet me ask you a different question then.\n\nHow many times have you looked at the result of your model and wondered **what-if** the data was something other than what I trained on.\nMaybe you write an algorithm that predicts the sales of comic books, and your model works really well and produces high accuracy predictions, but you need to know why.\nOr maybe its the opposite, your algorithm predicts completely wrong sales figures and you really need to figure out a reason for that.\n\nConfused? Let's look at an example- \n\nSuppose you are hired by Marvel as a data scientist. There has been a recent rise in comic book sales and you need to figure out the reason so that the company can mantain the sales figures. After some data analysis you come to the conclusion that there is a direct correlation between comic book sales and disney plus subscriptions. But you still don't have an exact reason, so you come up with some scenarios to form a hypotheses.\n\n![example1_scenario1](/assets/images/1post/eg1scenario1.png)\nThe comics display ads from disney, so obviously an increase in comic book sales causes more Disney Plus subscriptions.\n\n### Scenario 2\n![example1_scenario12](/assets/images/1post/eg1scenario2.png)\nDisney Plus has shows about comic book characters. New fans would like to consume more of such content. So increase in disney plus subscriptions lead to more marvel comics being bought.\n\n### Scenario 3\n![example1_scenario3](/assets/images/1post/eg1scenario3.png)\nOr maybe its something quite different. All Marvel Cinematic Universe movies are on Disney Plus. When a new Marvel movie comes out the hype around these characters lead to more comic book sales (and in turn more Disney Plus subscriptions).\n\n----\nThe possibility presented in scenario 3 is what is termed as a hidden cause.\n\n>Each node is a variable and each arrow shows the direction of causal connection.\n\nSuppose we build an accurate model to predict when the user will buy more comic books. But that is all that our model would do. When in reality, the actual problem statement could be better expressed by a question like-\n\n>What would the user have done if we had done something differently?\n\nSo you might be thinking, I get it the model doesn't always take hidden causes into account, but it still works. The predictions are still accurate.\n\nWell, it is actually considered really lucky to have our observed effect and causal effect in the same direction. More often than not we see the opposite in real world data.\n\n![](/assets/images/1post/expectationreality.png)\n\nLets take a look at another example to understand this better- \n\nSay suppose you have now taken up employment at a food ordering app as a Data Scientist. The product designer wants to introduce a new User Interface which she believes will engage more users. She has introduced the new UI to a select few people and wants you to make sense of the data that has been gathered. You decide to judge the the Old User Interface and the new one on a common criteria, which is whether the user orders after opening the app. Lets call this the _opened-and-ordered_ criteria. \n\n| UI Type | Ratio of users who  opened-and-ordered  to total users | Percentage |\n|---------|--------------------------------------------------------|------------|\n| Old UI  | 20/100                                                 | 20%        |\n| New UI  | 28/100                                                 | 28%        |\n\nBased on this data, it seems pretty straightforward. The new UI is the clear winner. But something very strange happens when you condition the data on high activity vs low activity users.\n\n|                                                          | Old UI      | New UI         |\n|----------------------------------------------------------|-------------|----------------|\n| Percentage of Opened-and-Ordered for High Activity Users | 15/80 = 25% | 14/60 = 23.33% |\n| Percentage of Opened-and-Ordered for Low Activity Users  | 2/20 = 10%  | 5/40 = 7.5%    |\n\n\nThis discrepancy of data on being modeled on different subset of the original data is called \"Simpson's Paradox\". Google this later. :wink:\n\n> \"But hey why does this discrepancy occur?\"\n\nWell maybe, since the old UI is already tried and tested it is shown to users only at a particular time of the day, around evening when app activity is generally high. The new UI might be shown at other times of the day when the number of lower activity users who are likely to order food is actually less. \n\nSo the Causal diagram may look something like this:\n\n![](/assets/images/1post/eg2scenario1.png)\n\nThe debate on what it means to cause something and how cause precedes effect has been going on forever. Most prominent contestants include but are not limited to -\n\n* Aristotle\n* Hume\n* Einstein\n* That French guy from Matrix 3\n* Judea Pearl and many more\n\nHowever sadly until recently, the science of causal inference was not considered formal mathematics or even something that could be quantified and studied. \n\nThis was remedied hugely by the contributions of Judea Pearl (Causal Graphical Models) and Donald Rubin (Rubin Causal Model).\n\nSo in conclusion, what we really learned here was that simply because two variables are correlated does not **necessarily** mean that one is the direct cause of the other. If we do not keep an eye out for hidden causes and their effects we can actually do more harm through our model.\n\nTherefore the idea of **what would happen** had something in the observation process been changed becomes an imperative question to ask.\n\nThis is called Counterfactual Thinking.\n\n>Counterfactual thinking, or specifically thinking about what-if scenarios is the very heart of causal inference.\n\n\nSome of the ways we can answer a counterfactual question are -\n\n1. Randomization\n2. Natural Experiments\n3. Conditioning\n\nInetrestingly as we go down the list, the methods become much harder to use. But as we go up, the methods are more foolproof and give better validation results. We will take a closer look at these and the various underlying methodologies in the next blogpost, till then keep learning and don't stop asking the question -\n\n> What-If?\n\n![](/assets/images/1post/what-if-strange.gif)\n\n### References\n\n* [Causal Inference in Online Systems by Amit Sharma](http://www.amitsharma.in/talk/causal-inference-online-systems/)\n* [An awesome playlist for Causal Learning](https://youtube.com/playlist?list=PLT3sJRyVaw-m9YBy3KRavOYuKyXikO4f6) ","srcMarkdownNoYaml":"\n\nSo what does Causal Inference really mean? What does it mean to cause? Is Causality really as simple as understanding cause and effect? Let us attempt to first understand few of the ground rules before we play the game of Causality.\n![correlation does not imply causation](/assets/images/1post/memecausal.png)\n\nIf you had a penny for every time you heard that line, you would probably be Bruce Wayne by now. But what does it mean? Why does correlation of two events **not imply** causality? What is up?\n\n>At this point you're probably wondering, yeah I have heard about Causality a few hundred times, but do I really need to care?\n\nLet me ask you a different question then.\n\nHow many times have you looked at the result of your model and wondered **what-if** the data was something other than what I trained on.\nMaybe you write an algorithm that predicts the sales of comic books, and your model works really well and produces high accuracy predictions, but you need to know why.\nOr maybe its the opposite, your algorithm predicts completely wrong sales figures and you really need to figure out a reason for that.\n\nConfused? Let's look at an example- \n\nSuppose you are hired by Marvel as a data scientist. There has been a recent rise in comic book sales and you need to figure out the reason so that the company can mantain the sales figures. After some data analysis you come to the conclusion that there is a direct correlation between comic book sales and disney plus subscriptions. But you still don't have an exact reason, so you come up with some scenarios to form a hypotheses.\n\n### Scenario 1\n![example1_scenario1](/assets/images/1post/eg1scenario1.png)\nThe comics display ads from disney, so obviously an increase in comic book sales causes more Disney Plus subscriptions.\n\n### Scenario 2\n![example1_scenario12](/assets/images/1post/eg1scenario2.png)\nDisney Plus has shows about comic book characters. New fans would like to consume more of such content. So increase in disney plus subscriptions lead to more marvel comics being bought.\n\n### Scenario 3\n![example1_scenario3](/assets/images/1post/eg1scenario3.png)\nOr maybe its something quite different. All Marvel Cinematic Universe movies are on Disney Plus. When a new Marvel movie comes out the hype around these characters lead to more comic book sales (and in turn more Disney Plus subscriptions).\n\n----\nThe possibility presented in scenario 3 is what is termed as a hidden cause.\n\n>Each node is a variable and each arrow shows the direction of causal connection.\n\nSuppose we build an accurate model to predict when the user will buy more comic books. But that is all that our model would do. When in reality, the actual problem statement could be better expressed by a question like-\n\n>What would the user have done if we had done something differently?\n\nSo you might be thinking, I get it the model doesn't always take hidden causes into account, but it still works. The predictions are still accurate.\n\nWell, it is actually considered really lucky to have our observed effect and causal effect in the same direction. More often than not we see the opposite in real world data.\n\n![](/assets/images/1post/expectationreality.png)\n\nLets take a look at another example to understand this better- \n\nSay suppose you have now taken up employment at a food ordering app as a Data Scientist. The product designer wants to introduce a new User Interface which she believes will engage more users. She has introduced the new UI to a select few people and wants you to make sense of the data that has been gathered. You decide to judge the the Old User Interface and the new one on a common criteria, which is whether the user orders after opening the app. Lets call this the _opened-and-ordered_ criteria. \n\n| UI Type | Ratio of users who  opened-and-ordered  to total users | Percentage |\n|---------|--------------------------------------------------------|------------|\n| Old UI  | 20/100                                                 | 20%        |\n| New UI  | 28/100                                                 | 28%        |\n\nBased on this data, it seems pretty straightforward. The new UI is the clear winner. But something very strange happens when you condition the data on high activity vs low activity users.\n\n|                                                          | Old UI      | New UI         |\n|----------------------------------------------------------|-------------|----------------|\n| Percentage of Opened-and-Ordered for High Activity Users | 15/80 = 25% | 14/60 = 23.33% |\n| Percentage of Opened-and-Ordered for Low Activity Users  | 2/20 = 10%  | 5/40 = 7.5%    |\n\n\nThis discrepancy of data on being modeled on different subset of the original data is called \"Simpson's Paradox\". Google this later. :wink:\n\n> \"But hey why does this discrepancy occur?\"\n\nWell maybe, since the old UI is already tried and tested it is shown to users only at a particular time of the day, around evening when app activity is generally high. The new UI might be shown at other times of the day when the number of lower activity users who are likely to order food is actually less. \n\nSo the Causal diagram may look something like this:\n\n![](/assets/images/1post/eg2scenario1.png)\n\nThe debate on what it means to cause something and how cause precedes effect has been going on forever. Most prominent contestants include but are not limited to -\n\n* Aristotle\n* Hume\n* Einstein\n* That French guy from Matrix 3\n* Judea Pearl and many more\n\nHowever sadly until recently, the science of causal inference was not considered formal mathematics or even something that could be quantified and studied. \n\nThis was remedied hugely by the contributions of Judea Pearl (Causal Graphical Models) and Donald Rubin (Rubin Causal Model).\n\nSo in conclusion, what we really learned here was that simply because two variables are correlated does not **necessarily** mean that one is the direct cause of the other. If we do not keep an eye out for hidden causes and their effects we can actually do more harm through our model.\n\nTherefore the idea of **what would happen** had something in the observation process been changed becomes an imperative question to ask.\n\nThis is called Counterfactual Thinking.\n\n>Counterfactual thinking, or specifically thinking about what-if scenarios is the very heart of causal inference.\n\n\nSome of the ways we can answer a counterfactual question are -\n\n1. Randomization\n2. Natural Experiments\n3. Conditioning\n\nInetrestingly as we go down the list, the methods become much harder to use. But as we go up, the methods are more foolproof and give better validation results. We will take a closer look at these and the various underlying methodologies in the next blogpost, till then keep learning and don't stop asking the question -\n\n> What-If?\n\n![](/assets/images/1post/what-if-strange.gif)\n\n### References\n\n* [Causal Inference in Online Systems by Amit Sharma](http://www.amitsharma.in/talk/causal-inference-online-systems/)\n* [An awesome playlist for Causal Learning](https://youtube.com/playlist?list=PLT3sJRyVaw-m9YBy3KRavOYuKyXikO4f6) "},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":false,"output-file":"2021-04-12-what-do-we-mean-when-we-talk-about-causal-inference.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","title":"What do we mean when we talk about Causal Inference?","date":"2021-04-12","author":"Ritwik Raha","tags":["causality"],"image":"/assets/images/1post/memecausal.png"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}