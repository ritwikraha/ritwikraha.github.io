{"title":"Mixture of Experts for Dummies","markdown":{"yaml":{"title":"Mixture of Experts for Dummies","date":"2023-12-07","author":"Ritwik Raha","tags":["math","natural-language-processing","tutorial"],"image":"/assets/images/mixture-of-experts.png"},"headingText":"Mixture of Experts for Dummies","containsRefs":false,"markdown":"\n\n\nThis post is meant as a tutorial to help one get started with the basic concept of a Mixture-of-Expert. It looks at various types of MoEs and their individual nuances. Read on to get into the mix!\n\n#### In the beginning...\n\nLarge Language Models (LLMs) are powerful neural networks that have achieved remarkable results in various natural language processing tasks. However, their massive size and computational complexity pose challenges for training and deployment. To address some of these challenges, Mixture of Experts (MoE) architecture has emerged as a promising technique.\n\n#### MoE, MoE?\n\nMoE or Mixture of Experts is a neural network architecture that divides a large model into smaller, specialized sub-models called experts. Each expert is trained to handle specific subtasks or input types. During inference, a gating network decides which expert(s) are best suited for each input, and only those experts are activated. This allows for more efficient computation and resource allocation compared to a single large model.\n\n#### How does a MoE work?\n\nMoE consists of three main components:\n- Expert networks: These are smaller sub-models trained on specific subtasks or input types.\n- Gating network: This network determines which expert(s) are best suited for each input. It considers features of the input and the capabilities of each expert.\n- Combiner: This component aggregates the outputs from the activated experts to produce the final output of the model.\n\nHere's a simplified breakdown of the MoE process:\n\n- Input: The LLM receives an input (e.g., text, code, etc.).\n- Feature extraction: Features are extracted from the input.\n- Gating network: The gating network analyzes the features and activates a small subset of experts.\n- Expert processing: Each activated expert processes the input and generates its own output.\n- Combining: The outputs from the activated experts are combined to produce the final output of the LLM.\n    - $$ \\text{Output} = \\sum_{i=1}^N \\text{Expert}_i(\\text{Input}) \\cdot \\text{Gating}(i) $$\n\n#### Vanilla MoE\n\nA Vanilla MoE is the simplest form of the architecture; it is a simple MoE with all the Experts switched on. We can get a brief idea from the image shown here:\n\n![vanilla-moe](https://github.com/ritwikraha/Notes-on-Papers/assets/44690292/64ae9bab-3a6f-433d-94c7-2dcff4b5f262)\n\n#### Sparse MoE:\n\nImagine a team of experts working on a complex problem. Each expert has unique knowledge and skills, but it's not efficient to involve all of them for every task.\n\nSparse MoE (Mixture of Experts):\nGoogle Brain proposed a solution: a network with many \"experts,\" but only a few are active for each task. This allows for a larger model capacity while saving resources.\n\n![sparse-moe](https://github.com/ritwikraha/Notes-on-Papers/assets/44690292/5641e470-3dc6-4de2-858c-f1279eef5846)\n\nGoal: Achieve \"single sample single expert processing.\" This means the model chooses one specific expert for each input during inference, saving computation.\nExample: Imagine a team of 100 experts. The model only activates 3 experts for a specific task, significantly reducing computational cost.\n\nDuring training, the model tends to favor \"earlier\" experts, making them more likely to be chosen. This leads to only a few experts being used effectively. This is called the \"Expert Balancing problem.\"\n\n#### Transformer MoE\n\nWhen models reached hundreds of billions of parameters, scaling became difficult.\nMoE (Mixture of Experts) resurfaced as an economical and practical solution.\nGoogle's GShard pioneered MoE integration with Transformers.\nSubsequent work like Switch Transformer and GLaM further improved the technique.\nMoE reduced LLM parameters from billions to trillions.\nGLaM's architecture:\n\n![transformer-moe](https://github.com/ritwikraha/Notes-on-Papers/assets/44690292/b949af5e-895f-4cd0-bc2e-ab9719299228)\n\n- MoE layers (position-wise) interweave with FFN layers in the Transformer encoder and decoder.\n- Top-2 routing in the Gating Network selects the two most likely experts.\n\n#### Lifelong-MoE\n\nTo tackle Lifelong Learning Google released Lifelong-MoE in May 2023. The model's Lifelong learning strategy includes the following steps:\n- Expand the number of Experts and the corresponding Gating dimensions.\n- Freeze the old Expert and the corresponding Gating dimension, and only train the new Expert.\n- Use the Output Regularization method to ensure that the new Expert inherits the knowledge learned in the past\n\n#### Quiz Time!\n\n<iframe\n  src=\"https://itempool.com/AlertAntelope519172/c/wjhVIvDe8lz\"\n  style=\"width:100%; height:300px;\"\n></iframe>\n\n#### Wrapping Up...\n\nThink of MoE as a group of scientists working on a complex project. Each scientist has their own expertise and focuses on a specific task. There is also a lead scientist, who chooses which project to push and which to halt based on their features.\n\nThis allows the entire project to be completed more efficiently and effectively.\n\nMoE is a relatively new technique in the field of LLMs, so the research is voluminious and growing each day. However, its potential benefits are significant, and it is expected to play a major role in the future development of LLMs.\n\n#### References and Acknowledgement\n- This work was developed along with [Aritra Roy Gosthipaty](https://twitter.com/ariG23498)\n- [The Next LLMs Development: Mixture-of-Experts with ... - AIFT](https://zhuanlan.zhihu.com/p/650394454)\n- [Mixture of Experts-Introduction - Abdulkader Helwan](https://abdulkaderhelwan.medium.com/mixture-of-experts-introduction-39f244a4ff05#bypass)\n- [Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models](https://arxiv.org/abs/2305.14705)\n- [Memory Augmented Language Models through Mixture of Word Experts](https://arxiv.org/abs/2311.10768)\n\n{% if page.comments %}\n<div id=\"disqus_thread\"></div>\n<script>\n    /**\n    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.\n    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */\n    var disqus_config = function () {\n    this.page.url = 'https://ritwikraha.github.io{{ page.url }}';  // Replace PAGE_URL with your page's canonical URL variable\n    this.page.identifier = 'https://'+'{{ page.id }}'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable\n    };\n  \n    (function() { // DON'T EDIT BELOW THIS LINE\n    var d = document, s = d.createElement('script');\n    s.src = 'https://ritwikraha-github-io.disqus.com/embed.js';\n    s.setAttribute('data-timestamp', +new Date());\n    (d.head || d.body).appendChild(s);\n    })();\n</script>\n<noscript>Please enable JavaScript to view the <a href=\"https://disqus.com/?ref_noscript\">comments powered by Disqus.</a></noscript>\n{% endif %} ","srcMarkdownNoYaml":"\n\n### Mixture of Experts for Dummies\n\nThis post is meant as a tutorial to help one get started with the basic concept of a Mixture-of-Expert. It looks at various types of MoEs and their individual nuances. Read on to get into the mix!\n\n#### In the beginning...\n\nLarge Language Models (LLMs) are powerful neural networks that have achieved remarkable results in various natural language processing tasks. However, their massive size and computational complexity pose challenges for training and deployment. To address some of these challenges, Mixture of Experts (MoE) architecture has emerged as a promising technique.\n\n#### MoE, MoE?\n\nMoE or Mixture of Experts is a neural network architecture that divides a large model into smaller, specialized sub-models called experts. Each expert is trained to handle specific subtasks or input types. During inference, a gating network decides which expert(s) are best suited for each input, and only those experts are activated. This allows for more efficient computation and resource allocation compared to a single large model.\n\n#### How does a MoE work?\n\nMoE consists of three main components:\n- Expert networks: These are smaller sub-models trained on specific subtasks or input types.\n- Gating network: This network determines which expert(s) are best suited for each input. It considers features of the input and the capabilities of each expert.\n- Combiner: This component aggregates the outputs from the activated experts to produce the final output of the model.\n\nHere's a simplified breakdown of the MoE process:\n\n- Input: The LLM receives an input (e.g., text, code, etc.).\n- Feature extraction: Features are extracted from the input.\n- Gating network: The gating network analyzes the features and activates a small subset of experts.\n- Expert processing: Each activated expert processes the input and generates its own output.\n- Combining: The outputs from the activated experts are combined to produce the final output of the LLM.\n    - $$ \\text{Output} = \\sum_{i=1}^N \\text{Expert}_i(\\text{Input}) \\cdot \\text{Gating}(i) $$\n\n#### Vanilla MoE\n\nA Vanilla MoE is the simplest form of the architecture; it is a simple MoE with all the Experts switched on. We can get a brief idea from the image shown here:\n\n![vanilla-moe](https://github.com/ritwikraha/Notes-on-Papers/assets/44690292/64ae9bab-3a6f-433d-94c7-2dcff4b5f262)\n\n#### Sparse MoE:\n\nImagine a team of experts working on a complex problem. Each expert has unique knowledge and skills, but it's not efficient to involve all of them for every task.\n\nSparse MoE (Mixture of Experts):\nGoogle Brain proposed a solution: a network with many \"experts,\" but only a few are active for each task. This allows for a larger model capacity while saving resources.\n\n![sparse-moe](https://github.com/ritwikraha/Notes-on-Papers/assets/44690292/5641e470-3dc6-4de2-858c-f1279eef5846)\n\nGoal: Achieve \"single sample single expert processing.\" This means the model chooses one specific expert for each input during inference, saving computation.\nExample: Imagine a team of 100 experts. The model only activates 3 experts for a specific task, significantly reducing computational cost.\n\nDuring training, the model tends to favor \"earlier\" experts, making them more likely to be chosen. This leads to only a few experts being used effectively. This is called the \"Expert Balancing problem.\"\n\n#### Transformer MoE\n\nWhen models reached hundreds of billions of parameters, scaling became difficult.\nMoE (Mixture of Experts) resurfaced as an economical and practical solution.\nGoogle's GShard pioneered MoE integration with Transformers.\nSubsequent work like Switch Transformer and GLaM further improved the technique.\nMoE reduced LLM parameters from billions to trillions.\nGLaM's architecture:\n\n![transformer-moe](https://github.com/ritwikraha/Notes-on-Papers/assets/44690292/b949af5e-895f-4cd0-bc2e-ab9719299228)\n\n- MoE layers (position-wise) interweave with FFN layers in the Transformer encoder and decoder.\n- Top-2 routing in the Gating Network selects the two most likely experts.\n\n#### Lifelong-MoE\n\nTo tackle Lifelong Learning Google released Lifelong-MoE in May 2023. The model's Lifelong learning strategy includes the following steps:\n- Expand the number of Experts and the corresponding Gating dimensions.\n- Freeze the old Expert and the corresponding Gating dimension, and only train the new Expert.\n- Use the Output Regularization method to ensure that the new Expert inherits the knowledge learned in the past\n\n#### Quiz Time!\n\n<iframe\n  src=\"https://itempool.com/AlertAntelope519172/c/wjhVIvDe8lz\"\n  style=\"width:100%; height:300px;\"\n></iframe>\n\n#### Wrapping Up...\n\nThink of MoE as a group of scientists working on a complex project. Each scientist has their own expertise and focuses on a specific task. There is also a lead scientist, who chooses which project to push and which to halt based on their features.\n\nThis allows the entire project to be completed more efficiently and effectively.\n\nMoE is a relatively new technique in the field of LLMs, so the research is voluminious and growing each day. However, its potential benefits are significant, and it is expected to play a major role in the future development of LLMs.\n\n#### References and Acknowledgement\n- This work was developed along with [Aritra Roy Gosthipaty](https://twitter.com/ariG23498)\n- [The Next LLMs Development: Mixture-of-Experts with ... - AIFT](https://zhuanlan.zhihu.com/p/650394454)\n- [Mixture of Experts-Introduction - Abdulkader Helwan](https://abdulkaderhelwan.medium.com/mixture-of-experts-introduction-39f244a4ff05#bypass)\n- [Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models](https://arxiv.org/abs/2305.14705)\n- [Memory Augmented Language Models through Mixture of Word Experts](https://arxiv.org/abs/2311.10768)\n\n{% if page.comments %}\n<div id=\"disqus_thread\"></div>\n<script>\n    /**\n    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.\n    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */\n    var disqus_config = function () {\n    this.page.url = 'https://ritwikraha.github.io{{ page.url }}';  // Replace PAGE_URL with your page's canonical URL variable\n    this.page.identifier = 'https://'+'{{ page.id }}'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable\n    };\n  \n    (function() { // DON'T EDIT BELOW THIS LINE\n    var d = document, s = d.createElement('script');\n    s.src = 'https://ritwikraha-github-io.disqus.com/embed.js';\n    s.setAttribute('data-timestamp', +new Date());\n    (d.head || d.body).appendChild(s);\n    })();\n</script>\n<noscript>Please enable JavaScript to view the <a href=\"https://disqus.com/?ref_noscript\">comments powered by Disqus.</a></noscript>\n{% endif %} "},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":false,"output-file":"2023-12-07-moe-for-dummies.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","title":"Mixture of Experts for Dummies","date":"2023-12-07","author":"Ritwik Raha","tags":["math","natural-language-processing","tutorial"],"image":"/assets/images/mixture-of-experts.png"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}