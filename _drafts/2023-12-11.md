---

layout: post
title:  "Generative Pretraining"
author: "Ritwik Raha"
prev_title: false
prev_link: false
next_title: false
next_link: false
tags: math, natural-language-processing, tutorial
comments: true
permalink: /moe-for-dummies
image: /assets/site_images/

---

## GPT-Based Models: A Comprehensive Research Summary

**1. Introduction:**

Generative Pre-trained Transformer (GPT) models are a class of large language models (LLMs) based on the Transformer architecture, pre-trained on massive amounts of unlabeled text data. These models are known for their impressive capabilities in generating human-quality text, translating languages, writing different kinds of creative content, and answering questions in an informative way.

**2. Technical Background:**

* **Transformer Architecture:** GPT models are built upon the Transformer architecture, a powerful neural network architecture introduced in 2017. This architecture employs self-attention mechanisms, allowing the model to focus on relevant parts of the input sequence during processing, leading to better context capture and improved performance on various natural language processing (NLP) tasks.

* **Pre-training:** GPT models are pre-trained on a massive dataset of unlabeled text, allowing them to learn general language patterns and representations. This pre-training step enables them to perform various NLP tasks without requiring additional fine-tuning.

**3. GPT Model Variants:**

OpenAI has released several GPT models, each with its own characteristics:

* **GPT-1:** Introduced in 2018, GPT-1 was the first model in the series and demonstrated promising results in text generation tasks.
* **GPT-2:** Released in February 2019, GPT-2 significantly improved upon GPT-1 in terms of performance and capabilities. Its ability to generate realistic and coherent text raised concerns about potential misuse and OpenAI opted not to release the full model.
* **GPT-3:** Launched in 2020, GPT-3 is a significantly larger and more powerful model than its predecessors. It boasts impressive capabilities in various NLP tasks, including text generation, translation, code generation, and question answering.
* **GPT-4:** This latest iteration is currently in development and is expected to surpass GPT-3 in terms of performance and capabilities.

**4. Applications:**

GPT-based models have a wide range of potential applications, including:

* **Creative writing:** GPT models can be used to generate creative text formats like poems, code, scripts, musical pieces, email, letters, etc.
* **Machine translation:** These models can accurately translate languages, making communication across different cultures easier.
* **Chatbots and virtual assistants:** GPT models can power chatbots and virtual assistants that can interact with humans in a natural and engaging way.
* **Content creation:** GPT models can be used to generate high-quality content for various purposes, such as marketing copy, social media posts, and news articles.
* **Question answering:** GPT models can be used to answer questions in a comprehensive and informative way, even for open ended, challenging, or strange questions.

**5. Research Areas:**

Active research areas surrounding GPT-based models include:

* **Explainability and interpretability:** Understanding how GPT models arrive at their outputs is crucial for building trust and ensuring responsible use.
* **Safety and bias:** Mitigating harmful biases and ensuring GPT models are used safely and ethically is an ongoing challenge.
* **Fine-tuning and customization:** Developing techniques to fine-tune GPT models for specific tasks and optimize their performance.
* **Model size and scaling:** Exploring the potential of scaling GPT models further to achieve even greater performance.

**6. Resources:**

* **OpenAI GPT Models:** [https://platform.openai.com/](https://platform.openai.com/)
* **Generative Pre-trained Transformer: Wikipedia:** [https://en.wikipedia.org/wiki/Generative_pre-trained_transformer](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer)
* **GPT-3: Wikipedia:** [https://en.wikipedia.org/wiki/GPT-3](https://en.wikipedia.org/wiki/GPT-3)
* **What is GPT AI? - Generative Pre-Trained Transformers Explained - AWS:** [https://aws.amazon.com/marketplace/pp/prodview-fk4xbj3ch2e4o](https://aws.amazon.com/marketplace/pp/prodview-fk4xbj3ch2e4o)
* **GPT models explained. Open AI's GPT-1,GPT-2,GPT-3 | Walmart Global Tech Blog - Medium:** [https://medium.com/swlh/understanding-gpt-3-openais-latest-language-model-a3ef89cffac2](https://medium.com/swlh/understanding-gpt-3-openais-latest-language-model-a3ef89cffac2)

**7. Conclusion:**

GPT-based models represent a significant advancement in NLP and artificial intelligence. Their ability to generate human-quality text and perform various NLP tasks has opened up vast opportunities for creative applications and technological advancement. However, it is crucial to address remaining research challenges to ensure the responsible and ethical development and deployment of these powerful models.


## GPT-4 Technical Report: A Detailed Breakdown

The GPT-4 technical report, published by OpenAI in March 2023, outlines the development and performance of this latest iteration of the GPT model family. Here's a detailed breakdown of the report, highlighting key architectural details and potential reasons behind its impressive performance:

**1. Architecture:**

* GPT-4 is a Transformer-based model, similar to its predecessors, but significantly larger and more complex. It boasts:
    * **10 trillion parameters:** This is a massive increase compared to GPT-3's 175 billion parameters, allowing for a more nuanced understanding of language patterns and relationships.
    * **Multimodal capabilities:** GPT-4 now accepts both image and text inputs, enabling it to generate text descriptions of images and respond to prompts based on both visual and textual information.
    * **Improved post-training alignment:** OpenAI's adversarial testing and ChatGPT programs were used to fine-tune GPT-4, resulting in better factuality, steerability, and adherence to desired behavior than previous versions.

**2. Performance:**

* GPT-4 exhibits remarkable performance on various benchmarks, surpassing GPT-3 and other large language models. Key highlights include:
    * **Passing a simulated bar exam with a score around the top 10% of test takers:** This demonstrates GPT-4's ability to learn complex information and apply its knowledge in challenging scenarios.
    * **Outperforming GPT-3 on traditional NLP benchmarks:** GPT-4 achieves state-of-the-art results in tasks like question answering, text summarization, and language translation.
    * **Generating human-quality text:** GPT-4 can produce creative and engaging text formats, indistinguishable from human-written content in many cases.

**3. Potential Reasons for Success:**

Several factors contribute to GPT-4's outstanding performance:

* **Massive dataset:** GPT-4 was trained on a dataset of text and code orders of magnitude larger than previous models, providing a richer and more comprehensive understanding of language.
* **Improved architecture:** The increased number of parameters and the multimodal capabilities allow for a more detailed and nuanced representation of language and information.
* **Advanced training techniques:** OpenAI's innovative training methods, including adversarial testing and post-training alignment, help mitigate biases and improve factual accuracy.
* **Scalable infrastructure:** OpenAI specifically designed a new supercomputer to handle the immense computational requirements of training such a massive model.


# What is Generative Pretraining?

At this point we have some idea of what is Generative Pretraining. 

It involves training a large language model (LLM) on a massive dataset of text and code (and sometimes other modalities) in an unsupervised manner. This means the model learns by itself, without needing any human-labeled examples.

Here's A short recap:



* Large Data: The LLM is first exposed to a vast amount of text data, such as books, articles, code, and online conversations. This data can be in various formats and styles, giving the model a broad understanding of language.
* Unsupervised Learning: Unlike traditional supervised learning, where the model is trained on labeled examples (e.g., "This is a cat", "This is a dog"), generative pre-training doesn't rely on such labels. Instead, it focuses on learning the underlying patterns and relationships within the data itself.
* Predictive Tasks: During training, the LLM is often tasked with predicting the next word in a sequence, given the previous words. This forces it to pay attention to context, grammar, and the overall flow of language.
* Self-Supervision: By continuously predicting the next word based on what it has already seen, the LLM essentially supervises its own learning. This iterative process allows it to refine its understanding of language and its ability to generate similar content.

Once pre-trained, the LLM can be fine-tuned for various NLP tasks, such as:



* **Text generation**: Writing different kinds of creative content, like poems, code, scripts, musical pieces, email, letters, etc.
* **Machine translation**: Translating text from one language to another.
* **Question answering**: Answering questions based on a given text or passage.
* **Text summarization**: Summarizing long pieces of text into shorter, concise summaries.
* **Sentiment analysis**: Determining the emotional tone of a piece of text.

Generative pre-training has revolutionized NLP by enabling models to achieve remarkable performance on various tasks. It has led to the development of powerful LLMs like GPT-4, Mxitral, Gemini, etc.

So now’s the big question, what do these models look like?

Are they just large transformers? Is there more layers and details inside? 

Well its safe to say, that its not like we don’t know anything about the architecture. There has been publicly released Large Language Models (LLMs) that match or atleast come close to the results of closed models like GPT-4 and Claude.


## Type of Architecture

In both GPT and GPT-2 models, they adopt a decoder-only transformer architecture. This design eliminates two key components from the transformer:

1. The complete encoder module.

2. All encoder-decoder self-attention modules within the decoder.

Once these components are taken out, each layer within the decoder is simplified to include a masked self-attention layer followed by a feedforward neural network. The stacking of multiple such layers creates a deep, decoder-only transformer architecture, as observed in models like GPT and GPT-2. See the description below for more details.



<p id="gdcalert1" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image1.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert2">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/image1.png "image_tooltip")


TODO: Create alternate diagram


## Why Decoder only Architecture?

The selection of the decoder architecture for Language Models (LMs) is deliberate, not random. Within the decoder, masked self-attention layers are crucial as they restrict the model from peeking ahead in a sequence when creating a token's representation. In contrast, bidirectional self-attention in the encoder allows each token's representation to adapt based on the entire sequence.

Language modeling necessitates masked self-attention to prevent forward-looking predictions. This design choice results in an autoregressive structure where the model predicts subsequent tokens in a sequence based on its previous outputs, as illustrated below.

GPT and GPT-2 mark a departure from the specialized expert models common in deep learning. Instead of training individual models for each application, a single LM is pre-trained and then adapted to handle various tasks. These versatile models, known as foundation models, address data scarcity issues by pre-training on extensive and diverse datasets. Moreover, they offer adaptability for solving different tasks, eliminating the need for frequent model retraining.

Fine-tuning, involving further training on supervised data, has traditionally been used to adapt a foundation model to specific tasks. However, contemporary methods increasingly favor zero or few-shot inference for this purpose.


## Ablations on the Transformer architecture


### RMSNorm in place of LayerNorm

In most transformer architectures, there's a technique called "layer normalization" used after each layer in the transformer block. This helps the model learn more effectively. However, LLaMA model, uses a different version called "Root Mean Square Layer Normalization" (RMSNorm). RMSNorm is a simpler form of layer normalization. It's been found to make training more stable and help the model generalize better, meaning it performs well on a wider range of tasks.

For LLaMA, a pre-normalization variant of RMSNorm is adopted, meaning that normalization is applied prior to the major layers in a transformer block, rather than after, as shown in the figure above



<p id="gdcalert2" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image2.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert3">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/image2.png "image_tooltip")


TODO: Create alternate diagram


### SwiGLU in place of ReLU



<p id="gdcalert3" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image3.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert4">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/image3.png "image_tooltip")


TODO: Create alternate diagram

SwiGLU is a technique used in neural networks. It works by taking an input (let's call it x) and applying two different linear transformations to it. A linear transformation is basically a way to change the input in a structured manner (like scaling or rotating in a geometric sense). After these transformations, one of the results is further modified by a Swish activation function. An activation function in neural networks helps decide how much a particular neuron should 'fire' or activate.

The Swish activation function makes SwiGLU more computationally demanding than simpler functions like ReLU (Rectified Linear Unit), because it requires three matrix multiplications (which are complex mathematical operations). Despite this extra computational cost, SwiGLU has been found to improve the performance of neural networks, meaning they can learn better or faster, even when the amount of computing power used is the same.


### Rope Positional Embeddings

Traditionally, models used either absolute positional embeddings (which tell the model exactly where each word is in a sentence) or relative positional embeddings (which tell the model how far apart words are from each other).

However, some models use a method called RoPE (Rotary Positional Embedding). RoPE is a mix of both absolute and relative approaches. It uses a rotation matrix to encode the absolute position of each token (like a word). Then, it adds information about the relative positions of tokens directly into the self-attention operation. This way, the model gets a more nuanced understanding of both where each token is and how it relates to others around it.


### Mixture of Experts (MoE)



<p id="gdcalert4" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image4.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert5">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/image4.png "image_tooltip")


TODO: Create alternate diagram

Mixture of Experts(MoE) will be covered separately in a later module along with code example. MoE is a neural network architecture that divides a large model into smaller, specialized sub-models called experts. Each expert is trained to handle specific subtasks or input types. During inference, a gating network decides which expert(s) are best suited for each input, and only those experts are activated. This allows for more efficient computation and resource allocation compared to a single large model.

How does a MoE work?

MoE consists of three main components:



* Expert networks: These are smaller sub-models trained on specific subtasks or input types.
* Gating network: This network determines which expert(s) are best suited for each input. It considers features of the input and the capabilities of each expert.
* Combiner: This component aggregates the outputs from the activated experts to produce the final output of the model.


## Pretraining on Quality Data 

The process of pre-training large models emphasizes critical attributes such as safety and beneficial characteristics, which are crucial for the resulting model's development. Notably, the focus on safety and data quality during pre-training starts early in the process. Certain sources known for containing personal information are excluded from the pre-training dataset, while emphasizing data from reputable and factual sources. Moreover, creators meticulously evaluate various elements, including the representation of diverse demographic groups and levels of undesirable content, aiming to reduce biases within the model. In essence, meticulous attention is given to curating and shaping the contents of the pre-training dataset for these large models.


## Fine Tuning and Alignment


### Supervised Fine Tuning

In contrast, models developed by private entities often undergo a comprehensive refinement process, involving extensive dialogue data and human input for fine-tuning. To bridge this disparity, efforts are made in models like LLaMA-2 to undergo a similar fine-tuning procedure using substantial datasets, resulting in models optimized for dialogue applications, such as the LLaMA-2-Chat model. 

The first step in the fine-tuning process is Supervised Fine Tuning or SFT. This technique relies upon a (supervised) next-token prediction objective that is nearly identical to pre-training. 

In other words, we just collect a dataset of dialogue examples and train the model using a next-token prediction objective applied to the responses within each of these examples. By fine-tuning the model in this way, the LLM learns to replicate behavior emulated within the responses of the fine-tuning dataset. Thus, any desirable properties—or alignment objectives—present within this data, in turn, become present within the model itself.


### Reinforcement Learning from Human Feedback (RLHF)

RLHF will be covered in a separate module in this course, explaining its origin, evolution and the current best practices. In this section we will be looking at a brief overview of the methodologies. 

While Self-Fine-Tuning (SFT) proves effective, ensuring that the dataset accurately reflects alignment goals can pose challenges in curation and management.

On the contrary, Reinforcement Learning from Human Feedback (RLHF), as detailed earlier, opts for a direct approach by fine-tuning the Large Language Models (LLMs) using human feedback on the model's output.

RLHF involves human annotators crafting prompts for LLMs, selected based on alignment criteria like safety and helpfulness. Subsequently, the LLM generates multiple responses for each prompt, and human annotators rank these responses based on quality, typically defined by alignment criteria adherence. RLHF utilizes these human preference scores to optimize the LLM.



<p id="gdcalert5" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image5.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert6">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/image5.png "image_tooltip")


TODO: Create alternate diagram


### Steps to RLHF:



* Data Collection
* Training the Reward model
* Optimization using RL


#### Data Collection

In data collection, open source models like LLAMA-2 adopts a binary protocol for gathering human preference data. 



* An annotator writes a prompt, leading to the generation of two model responses, and the annotator selects the preferred response based on safety and helpfulness criteria. 
* Each instance of human preference collection focuses singularly on a specific alignment criterion. For instance, an annotator may create an adversarial prompt aiming to elicit an unsafe response, then assess the responses based on safety.
* Similarly, detailed instructions prompt evaluation to determine which model response is more helpful.


#### Training the Reward Model



1. Training the Reward Model: After gathering human feedback, a reward model is trained. This model takes a conversation prompt and a response, then predicts how much people would prefer that response. It's similar to the large language model (LLM) but has a different final layer. Instead of predicting the next word, it predicts how good a response is.



<p id="gdcalert6" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image6.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert7">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/image6.png "image_tooltip")


TODO: Create alternate diagram



2. Automating Preference Scores: The reward model gives scores to different responses, which helps the LLM learn better through reinforcement learning. The model is trained to make the more preferred responses score higher.
3. Using Granular Feedback: The feedback isn't just "good" or "bad." It includes more nuanced categories like "significantly better" or "slightly better." By adding a margin to these categories in the training process, the model learns to understand the differences in preference more clearly.



<p id="gdcalert7" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image7.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert8">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/image7.png "image_tooltip")


TODO: Create alternate diagram



4. Combining Different Data Sources: The researchers mix their own preference data with other public data. This doesn't harm the model's performance and helps it generalize better. It also prevents "reward hacking," where the model might learn to trick the system into high scores without actually improving.


#### Optimization using RL



1. PPO Algorithm: PPO (Proximal Policy Optimization) is a common method used in RLHF (Reinforcement Learning from Human Feedback). It updates the model iteratively, using one sample from the model at a time.
2. Rejection Sampling: This is another method used alongside PPO. It works by generating multiple responses (K responses) from the LLM (Large Language Model) for each prompt. Then, it uses the reward model to score each response. The best response is selected and used for fine-tuning the model. Unlike PPO, which updates after each sample, rejection sampling generates a whole dataset of high-reward samples first, and then uses this dataset for fine-tuning.
3. Using Rejection Sampling with Large Models: Rejection sampling is particularly used with the largest model (like LLaMA-70B-Chat) to create high-quality training data. This data is then used to train smaller models.
4. Tweaking the Temperature: The 'temperature' setting, which affects how the model generates responses (like being more creative or sticking to facts), needs to be adjusted after each round of RLHF. The optimal temperature varies depending on the context of the prompts, like whether they are creative or factual.


### References:

* [https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up](https://cameronrwolfe.substack.com/p/llama-2-from-the-ground-up)
* [https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2#%C2%A7decoder-only-transformers](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2#%C2%A7decoder-only-transformers)
* [https://data-sleek.com/blog/gpt-4-all-you-need-to-know/#](https://data-sleek.com/blog/gpt-4-all-you-need-to-know/#:~:text=its%20inner%20workings.-,GPT%2D4%20Architecture%20and%20Training,dataset%20comprising%20text%20and%20code)
* [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)
* [https://openai.com/research/openai-baselines-ppo](https://openai.com/research/openai-baselines-ppo)
* [https://openai.com/research/language-models-can-explain-neurons-in-language-models](https://openai.com/research/language-models-can-explain-neurons-in-language-models)
* 

**5. Resources:**

* **OpenAI GPT-4 Technical Report:** [https://arxiv.org/abs/2303.08774](https://arxiv.org/abs/2303.08774)
* **OpenAI GPT Website:** [https://openai.com/](https://openai.com/)

**6. Conclusion:**

GPT-4 represents a significant leap forward in large language models, demonstrating remarkable performance across various NLP tasks. Its impressive capabilities stem from a combination of architectural advancements, massive training data, and cutting-edge training techniques. As research continues, we can expect even more powerful and sophisticated language models in the future, pushing the boundaries of artificial intelligence and its potential applications.


{% if page.comments %}
<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    var disqus_config = function () {
    this.page.url = 'https://ritwikraha.github.io{{ page.url }}';  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = 'https://'+'{{ page.id }}'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
  
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://ritwikraha-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
{% endif %}
